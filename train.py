import os
# å…è®¸é‡å¤çš„ OpenMP è¿è¡Œæ—¶ï¼ˆä¸æ¨èä½œä¸ºé•¿æœŸè§£å†³æ–¹æ¡ˆï¼Œåªæ˜¯ä¸´æ—¶å˜é€šï¼‰
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau
from datetime import datetime
import random

from models.unet3d import UNet3D
from dataloader import VolumeDataset
from models.AERB3d import AERBUNet3DLight
from models.attention_unet3d import LightAttentionUNet3D
from models.seunet3d import SEUNet3D
from models.attention_unet3d import AttentionUNet3D

# ============================================================================
# å¯ç¼–è¾‘æ¨¡å‹é…ç½®ï¼ˆåœ¨æ­¤å¤„ä¿®æ”¹ä»¥è®­ç»ƒä¸åŒæ¨¡å‹ï¼‰
# ============================================================================
MODEL_CONFIG = {
    "model_name": "attention_unet3d",  # å¯é€‰ 'unet3d', 'aerb_light', 'attn_light', 'seunet3d',attention_unet3d
    "in_channels": 1,
    "out_channels": 1,
    "base_channels": 16,
    "dropout_prob": 0.1,
    "pretrained_ckpt": None,
}

# ç®€å•æ¨¡å‹æ³¨å†Œè¡¨ä¸æ„é€ å™¨
_MODEL_REGISTRY = {
    "unet3d": UNet3D,
    "aerb_light": AERBUNet3DLight,
    "attn_light": LightAttentionUNet3D,
    "seunet3d": SEUNet3D,
    "attention_unet3d": AttentionUNet3D,
}


# ============================================================================
# è®¾ç½®éšæœºç§å­
# ============================================================================
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

# è®¾ç½®éšæœºç§å­
set_seed(42)

# ============================================================================
# ä¼˜åŒ–å™¨å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
# ============================================================================
def get_optimizer(model, lr=1e-4, optimizer_type='adamw', weight_decay=1e-4):
    """
    è·å–é€‚åˆ3Dåˆ†å‰²ç½‘ç»œçš„ä¼˜åŒ–å™¨
    
    Args:
        model: è¦ä¼˜åŒ–çš„æ¨¡å‹
        lr: åˆå§‹å­¦ä¹ ç‡
        optimizer_type: 'adamw', 'adam', 'sgd'
        weight_decay: L2æ­£åˆ™åŒ–å¼ºåº¦
    """
    if optimizer_type.lower() == 'adamw':
        # AdamW: æ›´å¥½çš„æƒé‡è¡°å‡å¤„ç†ï¼Œé€šå¸¸æ€§èƒ½æ›´å¥½
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            betas=(0.9, 0.999),      # åŠ¨é‡å‚æ•°
            eps=1e-8,                # æ•°å€¼ç¨³å®šæ€§
            weight_decay=weight_decay,  # L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            amsgrad=False            # é€šå¸¸ä¸éœ€è¦AMSGrad
        )
    elif optimizer_type.lower() == 'adam':
        # æ”¹è¿›çš„Adam
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=weight_decay  # æ·»åŠ æƒé‡è¡°å‡
        )
    elif optimizer_type.lower() == 'sgd':
        # SGD with momentum (å¯¹æŸäº›ä»»åŠ¡æœ‰æ•ˆ)
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=lr,
            momentum=0.9,           # åŠ¨é‡
            weight_decay=weight_decay,  # æƒé‡è¡°å‡
            nesterov=True           # NesterovåŠ¨é‡
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
    
    return optimizer

# ============================================================================
# è‡ªé€‚åº”è°ƒåº¦å™¨å‡½æ•°ï¼ˆé’ˆå¯¹ä¸åŒepochæ•°è‡ªåŠ¨ä¼˜åŒ–ï¼‰
# ============================================================================
def get_adaptive_scheduler(optimizer, warmup_epochs=None, total_epochs=400, scheduler_type='auto'):
    """
    è‡ªé€‚åº”è°ƒåº¦å™¨ï¼šæ ¹æ®æ€»epochæ•°è‡ªåŠ¨è°ƒæ•´å‚æ•°
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        warmup_epochs: æš–åœºepochæ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
        total_epochs: æ€»è®­ç»ƒepochæ•°
        scheduler_type: 'auto'ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰ã€'cosine_warm'ã€'cosine'ã€'plateau'ã€'step'
    
    Returns:
        scheduler: ä¸»è°ƒåº¦å™¨
        plateau_scheduler: å¦‚æœæ˜¯plateauç±»å‹ï¼Œè¿”å›é¢å¤–çš„è°ƒåº¦å™¨
    """
    
    # ========== è‡ªåŠ¨è®¡ç®—æœ€ä½³å‚æ•° ==========
    if warmup_epochs is None:
        # æ ¹æ®æ€»epochæ•°è‡ªåŠ¨è®¡ç®—æš–åœºepochs
        if total_epochs <= 30:
            warmup_epochs = max(2, total_epochs // 10)     # æœ€å°‘2ä¸ªepoch
        elif total_epochs <= 100:
            warmup_epochs = max(5, total_epochs // 10)     # 10%å·¦å³
        elif total_epochs <= 200:
            warmup_epochs = max(10, total_epochs // 15)    # çº¦5-7%
        else:
            warmup_epochs = max(15, total_epochs // 20)    # çº¦5%
    
    # ========== è‡ªåŠ¨é€‰æ‹©è°ƒåº¦å™¨ç±»å‹ ==========
    if scheduler_type == 'auto':
        # æ ¹æ®epochæ•°è‡ªåŠ¨é€‰æ‹©æœ€ä½³è°ƒåº¦å™¨
        if total_epochs <= 30:
            scheduler_type = 'cosine'      # çŸ­è®­ç»ƒç”¨å•å‘¨æœŸä½™å¼¦
        elif total_epochs <= 100:
            scheduler_type = 'cosine_warm' # ä¸­ç­‰è®­ç»ƒç”¨å¤šå‘¨æœŸä½™å¼¦
        else:
            scheduler_type = 'plateau'     # é•¿è®­ç»ƒç”¨è‡ªé€‚åº”è°ƒåº¦
    
    print(f"è‡ªé€‚åº”é…ç½®: {total_epochs} epochs -> "
          f"warmup={warmup_epochs}, scheduler={scheduler_type}")
    
    # ========== æš–åœºé˜¶æ®µ ==========
    warmup_scheduler = LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=warmup_epochs
    )
    
    # ========== ä¸»è°ƒåº¦å™¨ï¼ˆæ ¹æ®epochæ•°è‡ªé€‚åº”ï¼‰ ==========
    plateau_scheduler = None
    
    if scheduler_type == 'cosine_warm':
        # å¤šå‘¨æœŸä½™å¼¦é€€ç«ï¼Œè‡ªåŠ¨è°ƒæ•´å‘¨æœŸé•¿åº¦
        if total_epochs <= 50:
            T_0 = max(10, (total_epochs - warmup_epochs) // 3)
            T_mult = 1
            eta_min = 1e-5
        elif total_epochs <= 100:
            T_0 = max(15, (total_epochs - warmup_epochs) // 4)
            T_mult = 2
            eta_min = 1e-6
        elif total_epochs <= 200:
            T_0 = max(20, (total_epochs - warmup_epochs) // 5)
            T_mult = 2
            eta_min = 1e-6
        else:
            T_0 = max(30, (total_epochs - warmup_epochs) // 6)
            T_mult = 2
            eta_min = 1e-7
        
        main_scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=T_0,
            T_mult=T_mult,
            eta_min=eta_min,
            last_epoch=-1
        )
        
    elif scheduler_type == 'cosine':
        # å•å‘¨æœŸä½™å¼¦é€€ç«ï¼Œè°ƒæ•´æœ€å°å­¦ä¹ ç‡
        if total_epochs <= 30:
            eta_min = 1e-5
        elif total_epochs <= 100:
            eta_min = 1e-6
        else:
            eta_min = 1e-7
            
        main_scheduler = CosineAnnealingLR(
            optimizer,
            T_max=total_epochs - warmup_epochs,
            eta_min=eta_min
        )
        
    elif scheduler_type == 'plateau':
        # ReduceLROnPlateauï¼Œæ ¹æ®epochæ•°è°ƒæ•´è€å¿ƒå€¼
        patience = max(5, total_epochs // 20)  # è‡ªåŠ¨è®¡ç®—è€å¿ƒå€¼
        main_scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=patience,
            min_lr=1e-7,
            verbose=True,
            threshold=1e-4
        )
        plateau_scheduler = main_scheduler
        main_scheduler = warmup_scheduler
        
    elif scheduler_type == 'step':
        # é˜¶æ¢¯å¼ä¸‹é™
        step_size = max(20, total_epochs // 5)  # è‡ªåŠ¨è®¡ç®—æ­¥é•¿
        main_scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=step_size,
            gamma=0.5
        )
    
    # ========== ç»„åˆè°ƒåº¦å™¨ ==========
    if scheduler_type != 'plateau':
        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[warmup_epochs]
        )
        return scheduler, plateau_scheduler
    else:
        return main_scheduler, plateau_scheduler


def build_model_from_config(cfg):
    """
    æ ¹æ® MODEL_CONFIG æ„é€ æ¨¡å‹å®ä¾‹å¹¶ï¼ˆå¯é€‰ï¼‰åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚
    ç›´æ¥ç¼–è¾‘ä¸Šæ–¹ MODEL_CONFIG å³å¯åˆ‡æ¢æ¨¡å‹ä¸å‚æ•°ã€‚
    """
    name = cfg.get("model_name", "unet3d")
    cls = _MODEL_REGISTRY.get(name)
    if cls is None:
        raise ValueError(f"Unknown model_name '{name}'. Available: {list(_MODEL_REGISTRY.keys())}")

    kwargs = {}
    # å¸¸è§æ„é€ å‚æ•°ï¼ŒæŒ‰éœ€ä¼ å…¥
    if "in_channels" in cfg:
        kwargs["in_channels"] = cfg["in_channels"]
    if "out_channels" in cfg:
        kwargs["out_channels"] = cfg["out_channels"]
    if "base_channels" in cfg:
        # ä¸€äº›æ¨¡å‹ä½¿ç”¨ base_channels å‚æ•°åä¸º base_channels æˆ– base_num
        kwargs["base_channels"] = cfg["base_channels"]
    if "dropout_prob" in cfg:
        kwargs["dropout_prob"] = cfg["dropout_prob"]

    # å®ä¾‹åŒ–
    model = cls(**{k: v for k, v in kwargs.items() if k in cls.__init__.__code__.co_varnames})

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆè‹¥æä¾›è·¯å¾„ï¼‰
    ckpt = cfg.get("pretrained_ckpt")
    if ckpt:
        ckpt_path = Path(ckpt)
        if ckpt_path.exists():
            sd = torch.load(str(ckpt_path), map_location="cpu")
            # æ”¯æŒå¸¸è§åŒ…è£…å­—æ®µ
            if isinstance(sd, dict) and "model_state" in sd:
                sd = sd["model_state"]
            if isinstance(sd, dict) and "state_dict" in sd:
                sd = sd["state_dict"]
            # å»æ‰ module. å‰ç¼€ï¼ˆDataParallelï¼‰
            new_sd = {}
            for k, v in sd.items():
                nk = k[len("module."):] if k.startswith("module.") else k
                new_sd[nk] = v
            try:
                model.load_state_dict(new_sd, strict=False)
                print(f"Loaded pretrained weights from {ckpt_path} (strict=False)")
            except Exception as e:
                print(f"Warning: failed to fully load pretrained weights: {e}")
        else:
            print(f"Warning: pretrained_ckpt path does not exist: {ckpt_path}")

    return model

# ============================================================================
# æŸå¤±å‡½æ•°
# ============================================================================
class DiceLoss(nn.Module):
    def __init__(self, eps=1e-6):
        super(DiceLoss, self).__init__()
        self.eps = eps

    def forward(self, logits, targets):
        probs = torch.sigmoid(logits)
        num = 2 * (probs * targets).sum(dim=(1, 2, 3, 4))
        den = probs.sum(dim=(1, 2, 3, 4)) + targets.sum(dim=(1, 2, 3, 4))
        dice = (num + self.eps) / (den + self.eps)
        return 1 - dice.mean()

# ============================================================================
# è¿‡æ‹Ÿåˆæ£€æµ‹å™¨
# ============================================================================
class OverfittingDetector:
    """
    æ£€æµ‹è¿‡æ‹Ÿåˆçš„ç±»
    æ£€æµ‹æ ‡å‡†ï¼š
    1. éªŒè¯æŸå¤±è¿ç»­ä¸Šå‡ï¼ˆç›¸å¯¹äºæœ€ä½ç‚¹ï¼‰
    2. è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å·®è·è¿‡å¤§
    3. Diceç³»æ•°ç­‰æŒ‡æ ‡å‡ºç°åˆ†æ­§
    """
    def __init__(self, patience=10, min_delta=1e-4, gap_threshold=0.5, history_window=5):
        """
        Args:
            patience: å®¹å¿éªŒè¯æŸå¤±ä¸ä¸‹é™çš„epochæ•°
            min_delta: éªŒè¯æŸå¤±çš„æœ€å°æ”¹å–„é˜ˆå€¼
            gap_threshold: è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å·®è·çš„é˜ˆå€¼
            history_window: ç”¨äºè®¡ç®—è¶‹åŠ¿çš„çª—å£å¤§å°
        """
        self.patience = patience
        self.min_delta = min_delta
        self.gap_threshold = gap_threshold
        self.history_window = history_window
        
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.counter = 0
        
        # è®°å½•å†å²
        self.train_losses = []
        self.val_losses = []
        self.train_val_gaps = []
        
    def update(self, epoch, train_loss, val_loss):
        """æ›´æ–°æ£€æµ‹å™¨çŠ¶æ€"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        gap = abs(train_loss - val_loss)
        self.train_val_gaps.append(gap)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³éªŒè¯æŸå¤±
        if val_loss < self.best_val_loss - self.min_delta:
            self.best_val_loss = val_loss
            self.best_epoch = epoch
            self.counter = 0
            improved = True
        else:
            self.counter += 1
            improved = False
            
        return improved, self.check_overfitting(epoch)
    
    def check_overfitting(self, epoch):
        """æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ"""
        overfitting_signals = []
        
        # 1. æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦è¿ç»­ä¸Šå‡
        if len(self.val_losses) >= self.history_window:
            recent_val = self.val_losses[-self.history_window:]
            val_trend = np.polyfit(range(len(recent_val)), recent_val, 1)[0]
            if val_trend > 0.01:  # æ­£æ–œç‡ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
                overfitting_signals.append(f"éªŒè¯æŸå¤±è¿ç»­ä¸Šå‡ (è¶‹åŠ¿: {val_trend:.4f})")
        
        # 2. æ£€æŸ¥è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±çš„å·®è·
        if len(self.train_val_gaps) > 0:
            recent_gap = self.train_val_gaps[-1]
            if recent_gap > self.gap_threshold:
                overfitting_signals.append(f"è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·è¿‡å¤§ ({recent_gap:.4f} > {self.gap_threshold})")
        
        # 3. æ£€æŸ¥è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸ä¸‹é™
        if len(self.train_losses) >= self.history_window:
            recent_train = self.train_losses[-self.history_window:]
            train_trend = np.polyfit(range(len(recent_train)), recent_train, 1)[0]
            if train_trend < -0.01 and self.counter >= self.patience // 2:
                overfitting_signals.append("è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ä½†éªŒè¯æŸå¤±åœæ»")
        
        return overfitting_signals
    
    def get_summary(self):
        """è·å–æ£€æµ‹å™¨çŠ¶æ€æ‘˜è¦"""
        if len(self.train_losses) == 0:
            return "æ— è®­ç»ƒå†å²"
        
        summary = [
            f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f} (epoch {self.best_epoch})",
            f"å½“å‰éªŒè¯æŸå¤±: {self.val_losses[-1]:.6f}",
            f"å½“å‰è®­ç»ƒæŸå¤±: {self.train_losses[-1]:.6f}",
            f"è®­ç»ƒ-éªŒè¯å·®è·: {self.train_val_gaps[-1]:.6f}",
            f"è¿ç»­æœªæ”¹å–„epochæ•°: {self.counter}/{self.patience}"
        ]
        
        # æ·»åŠ è¶‹åŠ¿ä¿¡æ¯
        if len(self.val_losses) >= 3:
            recent_val = self.val_losses[-3:]
            val_trend = np.polyfit(range(len(recent_val)), recent_val, 1)[0]
            trend_symbol = 'â†‘' if val_trend > 0.001 else ('â†“' if val_trend < -0.001 else 'â†’')
            summary.append(f"éªŒè¯æŸå¤±è¶‹åŠ¿: {trend_symbol} ({val_trend:.6f})")
        
        return "\n".join(summary)
    
    def should_stop(self, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        overfitting_signals = self.check_overfitting(epoch)
        # ä¸»è¦åœæ­¢æ¡ä»¶ï¼šéªŒè¯æŸå¤±è¿ç»­patienceæ¬¡æœªæ”¹å–„
        should_stop = (self.counter >= self.patience)
        return should_stop, overfitting_signals

# ============================================================================
# å¯è§†åŒ–å·¥å…·
# ============================================================================
def plot_training_curves(train_losses, val_losses, learning_rates=None, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    if learning_rates is None:
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    else:
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    ax = axes[0] if learning_rates is None else axes[0]
    ax.plot(epochs, train_losses, 'b-', label='Train Loss', alpha=0.7, linewidth=2)
    ax.plot(epochs, val_losses, 'r-', label='Val Loss', alpha=0.7, linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Training and Validation Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # å·®è·æ›²çº¿
    ax = axes[1] if learning_rates is None else axes[1]
    if len(train_losses) > 0 and len(val_losses) > 0:
        gaps = [abs(t - v) for t, v in zip(train_losses, val_losses)]
        ax.plot(epochs, gaps, 'g-', label='Train-Val Gap', alpha=0.7, linewidth=2)
        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Warning Threshold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Gap')
        ax.set_title('Train-Validation Gap')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    if learning_rates is not None:
        ax = axes[2]
        # ä½¿ç”¨æ˜¾å¼ color/linestyleï¼Œé¿å…åƒ 'purple-' è¿™æ ·çš„éæ³• format string
        ax.plot(epochs, learning_rates, color='purple', linestyle='-', label='Learning Rate', alpha=0.7, linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.set_title('Learning Rate Schedule')
        ax.set_yscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

# ============================================================================
# å½’ä¸€åŒ–å‡½æ•°ï¼ˆé²æ£’ç‰ˆæœ¬ï¼‰
# ============================================================================
def _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)):
    """
    é²æ£’Z-scoreæ ‡å‡†åŒ–ï¼šä½¿ç”¨ä¸­ä½æ•°å’ŒMADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
    å¯¹å¼‚å¸¸å€¼å’Œé¢†åŸŸåç§»æ›´é²æ£’
    
    Args:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, C, Z, Y, X) æˆ– (B, Z, Y, X)
        use_mad: æ˜¯å¦ä½¿ç”¨MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰ä»£æ›¿æ ‡å‡†å·®
        clip_range: æˆªæ–­èŒƒå›´
    """
    if isinstance(x, torch.Tensor):
        x_np = x.cpu().numpy()
    else:
        x_np = np.asarray(x)
    
    original_shape = x_np.shape
    original_ndim = x_np.ndim
    
    # é‡å¡‘ä»¥ä¾¿äºè®¡ç®—
    if original_ndim == 5:
        # (B, C, Z, Y, X) -> (B*C, Z, Y, X)
        spatial_axes = (2, 3, 4)
        batch_channel_axis = (0, 1)
        n_samples = original_shape[0] * original_shape[1]
        reshaped_shape = (n_samples,) + original_shape[2:]
        x_reshaped = x_np.reshape(reshaped_shape)
    elif original_ndim == 4:
        # (B, Z, Y, X) -> (B, Z, Y, X)
        spatial_axes = (1, 2, 3)
        batch_axis = (0,)
        n_samples = original_shape[0]
        x_reshaped = x_np
    else:
        raise ValueError('Unsupported input ndim for normalization: %d' % original_ndim)
    
    # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„
    x_norm = np.zeros_like(x_reshaped, dtype=np.float32)
    
    # å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è¿›è¡Œé²æ£’å½’ä¸€åŒ–
    for i in range(n_samples):
        sample = x_reshaped[i]
        
        # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä¸­å¿ƒï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
        center = np.median(sample)
        
        if use_mad:
            # ä½¿ç”¨MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰ä½œä¸ºå°ºåº¦ä¼°è®¡
            # MAD = median(|x - median(x)|)
            mad = np.median(np.abs(sample - center))
            # å°†MADè½¬æ¢ä¸ºæ ‡å‡†å·®ä¼°è®¡: Ïƒ â‰ˆ 1.4826 * MADï¼ˆå¯¹äºæ­£æ€åˆ†å¸ƒï¼‰
            scale = mad * 1.4826 if mad > 1e-6 else 1.0
        else:
            # ä½¿ç”¨æ ‡å‡†å·®ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰
            scale = np.std(sample)
            if scale < 1e-6:
                scale = 1.0
        
        # é²æ£’Z-scoreæ ‡å‡†åŒ–
        sample_norm = (sample - center) / scale
        
        # æˆªæ–­å¼‚å¸¸å€¼
        if clip_range:
            sample_norm = np.clip(sample_norm, clip_range[0], clip_range[1])
        
        x_norm[i] = sample_norm
    
    # æ¢å¤åŸå§‹å½¢çŠ¶
    if original_ndim == 5:
        x_norm = x_norm.reshape(original_shape)
    
    return torch.from_numpy(x_norm).float()

def _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)):
    """
    ä¼ ç»ŸZ-scoreæ ‡å‡†åŒ–ï¼ˆä¿æŒåŸæœ‰å‡½æ•°ä»¥ä½œå¯¹æ¯”ï¼‰
    """
    if isinstance(x, torch.Tensor):
        x = x.cpu().numpy()
    x = np.asarray(x)
    if x.ndim == 5:
        spatial_axes = (2, 3, 4)
    elif x.ndim == 4:
        spatial_axes = (1, 2, 3)
    else:
        raise ValueError('Unsupported input ndim for normalization: %d' % x.ndim)

    mean = x.mean(axis=spatial_axes, keepdims=True)
    std = x.std(axis=spatial_axes, keepdims=True)
    std[std < 1e-6] = 1.0
    x_norm = (x - mean) / std
    
    if clip_range:
        x_norm = np.clip(x_norm, clip_range[0], clip_range[1])
    
    return torch.from_numpy(x_norm).float()

def _batch_iou(logits, targets, threshold=0.5, eps=1e-7):
    preds = (torch.sigmoid(logits) > threshold)
    targets_bool = targets.bool()
    intersection = (preds & targets_bool).sum(dim=(1, 2, 3, 4)).float()
    union = (preds | targets_bool).sum(dim=(1, 2, 3, 4)).float()
    iou = (intersection + eps) / (union + eps)
    return iou.mean()

# ============================================================================
# å½’ä¸€åŒ–æ•ˆæœåˆ†æå‡½æ•°
# ============================================================================
def analyze_normalization_effect(loader, use_robust_norm=True, num_batches=5):
    """
    åˆ†æå½’ä¸€åŒ–æ•ˆæœ
    """
    print("\n" + "="*60)
    print("å½’ä¸€åŒ–æ•ˆæœåˆ†æ")
    print("="*60)
    
    before_stats = {'mean': [], 'std': [], 'median': [], 'mad': [], 'min': [], 'max': []}
    after_stats = {'mean': [], 'std': [], 'median': [], 'mad': [], 'min': [], 'max': []}
    
    for i, (x, _) in enumerate(loader):
        if i >= num_batches:
            break
            
        # åŸå§‹ç»Ÿè®¡
        x_np = x.numpy()
        before_stats['mean'].append(np.mean(x_np))
        before_stats['std'].append(np.std(x_np))
        before_stats['median'].append(np.median(x_np))
        before_stats['mad'].append(np.median(np.abs(x_np - np.median(x_np))))
        before_stats['min'].append(np.min(x_np))
        before_stats['max'].append(np.max(x_np))
        
        # å½’ä¸€åŒ–åç»Ÿè®¡
        if use_robust_norm:
            x_norm = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).numpy()
        else:
            x_norm = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).numpy()
        
        after_stats['mean'].append(np.mean(x_norm))
        after_stats['std'].append(np.std(x_norm))
        after_stats['median'].append(np.median(x_norm))
        after_stats['mad'].append(np.median(np.abs(x_norm - np.median(x_norm))))
        after_stats['min'].append(np.min(x_norm))
        after_stats['max'].append(np.max(x_norm))
    
    print("\nå½’ä¸€åŒ–å‰åå¯¹æ¯”:")
    print("="*60)
    print("             å½’ä¸€åŒ–å‰                å½’ä¸€åŒ–å")
    print("="*60)
    
    for stat_name in ['mean', 'std', 'median', 'mad', 'min', 'max']:
        before_avg = np.mean(before_stats[stat_name])
        before_std = np.std(before_stats[stat_name])
        after_avg = np.mean(after_stats[stat_name])
        after_std = np.std(after_stats[stat_name])
        
        print(f"{stat_name:6s}: {before_avg:8.4f} Â± {before_std:.4f}  ->  {after_avg:8.4f} Â± {after_std:.4f}")
    
    print("="*60)
    print(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_norm else 'ä¼ ç»ŸZ-score'}")
    print(f"æˆªæ–­èŒƒå›´: {(-4.0, 4.0) if use_robust_norm else (-3.0, 3.0)}")
    
    return before_stats, after_stats

# ============================================================================
# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¸¦æ¢¯åº¦è£å‰ªï¼‰
# ============================================================================
def train_epoch(model, loader, opt, criterion_bce, criterion_dice, 
                device, scaler=None, accum_steps=1, max_grad_norm=1.0,
                use_robust_norm=True):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒ…å«æ¢¯åº¦è£å‰ª
    
    Args:
        max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
        use_robust_norm: æ˜¯å¦ä½¿ç”¨é²æ£’å½’ä¸€åŒ–
    """
    model.train()
    running_loss = 0.0
    running_bce = 0.0
    running_dice = 0.0
    running_iou = 0.0
    opt.zero_grad()
    
    for step, (x, y) in enumerate(tqdm(loader, desc='train', leave=False), start=1):
        # å½’ä¸€åŒ–ï¼ˆä½¿ç”¨é²æ£’å½’ä¸€åŒ–ï¼‰
        if use_robust_norm:
            x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).to(device)
        else:
            x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).to(device)
        
        y = y.float().to(device)

        # æ··åˆç²¾åº¦å‰å‘
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(x)
                loss_bce = criterion_bce(logits, y)
                loss_dice = criterion_dice(logits, y)
                loss =  0.3 * loss_bce + 0.7 * loss_dice  # åŠ æƒæŸå¤±
                batch_iou = _batch_iou(logits, y)
            scaler.scale(loss / accum_steps).backward()
        else:
            logits = model(x)
            loss_bce = criterion_bce(logits, y)
            loss_dice = criterion_dice(logits, y)
            loss =  0.3 * loss_bce + 0.7 * loss_dice  # æ›´å¼ºè°ƒDiceæŸå¤±
            batch_iou = _batch_iou(logits, y)
            (loss / accum_steps).backward()

        running_loss += loss.item() * x.size(0)
        running_bce += loss_bce.item() * x.size(0)
        running_dice += loss_dice.item() * x.size(0)
        running_iou += batch_iou.item() * x.size(0)

        # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ accum_steps æ­¥æ›´æ–°ä¸€æ¬¡
        if (step % accum_steps) == 0:
            if scaler is not None:
                # æ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„æ¢¯åº¦è£å‰ª
                scaler.unscale_(opt)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(opt)
                scaler.update()
            else:
                # æ™®é€šè®­ç»ƒæ—¶çš„æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                opt.step()
            opt.zero_grad()
    
    # è‹¥æœ€åä¸è¶³ accum_stepsï¼Œä¹Ÿè¦æ‰§è¡Œä¸€æ¬¡ stepï¼ˆå®‰å…¨å¤„ç†ï¼‰
    if len(loader) % accum_steps != 0:
        if scaler is not None:
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(opt)
            scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            opt.step()
        opt.zero_grad()
    
    total = len(loader.dataset)
    return {
        'total_loss': running_loss / total,
        'bce_loss': running_bce / total,
        'dice_loss': running_dice / total,
        'iou': running_iou / total
    }

def validate(model, loader, criterion_bce, criterion_dice, device, use_robust_norm=True):
    model.eval()
    running_loss = 0.0
    running_bce = 0.0
    running_dice = 0.0
    running_iou = 0.0
    
    with torch.no_grad():
        for x, y in tqdm(loader, desc='val', leave=False):
            # å½’ä¸€åŒ–ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•ï¼‰
            if use_robust_norm:
                x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).to(device)
            else:
                x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).to(device)
            
            y = y.float().to(device)
            logits = model(x)
            loss_bce = criterion_bce(logits, y)
            loss_dice = criterion_dice(logits, y)
            loss = 0.3 * loss_bce + 0.7 * loss_dice  # ä¸è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´
            batch_iou = _batch_iou(logits, y)
            running_loss += loss.item() * x.size(0)
            running_bce += loss_bce.item() * x.size(0)
            running_dice += loss_dice.item() * x.size(0)
            running_iou += batch_iou.item() * x.size(0)
    
    total = len(loader.dataset)
    return {
        'total_loss': running_loss / total,
        'bce_loss': running_bce / total,
        'dice_loss': running_dice / total,
        'iou': running_iou / total
    }

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    # ========== è®­ç»ƒé…ç½® ==========
    root = Path('.')
    epochs = 400               # æ€»è®­ç»ƒepochæ•°ï¼ˆè‡ªé€‚åº”è°ƒåº¦å™¨ä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰
    batch_size = 8               # æ‰¹å¤§å°
    lr = 1e-4                    # åˆå§‹å­¦ä¹ ç‡
    workers = 4                  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ========== å½’ä¸€åŒ–é…ç½® ==========
    use_robust_normalization = True  # æ˜¯å¦ä½¿ç”¨é²æ£’å½’ä¸€åŒ–ï¼ˆæ¨èTrueï¼‰
    norm_clip_range = (-4.0, 4.0) if use_robust_normalization else (-3.0, 3.0)
    
    # ========== ä¼˜åŒ–å™¨é…ç½® ==========
    optimizer_type = 'adamw'        # å¯é€‰: 'adamw', 'adam', 'sgd'
    weight_decay = 1e-4             # L2æ­£åˆ™åŒ–å¼ºåº¦
    max_grad_norm = 1.0             # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    
    # ========== è°ƒåº¦å™¨é…ç½® ==========
    scheduler_type = 'auto'         # 'auto'ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰ã€'cosine_warm'ã€'cosine'ã€'plateau'ã€'step'
    warmup_epochs = None            # Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
    
    # ========== è¿‡æ‹Ÿåˆæ£€æµ‹å‚æ•° ==========
    overfit_patience = 500           # å®¹å¿éªŒè¯æŸå¤±ä¸ä¸‹é™çš„epochæ•°
    overfit_min_delta = 1e-4        # æœ€å°æ”¹å–„é˜ˆå€¼
    overfit_gap_threshold = 0.5     # è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·é˜ˆå€¼
    
    # ========== å…¶ä»–è®­ç»ƒé…ç½® ==========
    use_amp = True                  # æ··åˆç²¾åº¦è®­ç»ƒ
    accum_steps = 1                 # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    
    # æ•°æ®æ ¼å¼é…ç½®
    dat_dtype = 'float32'
    dat_shape = (128, 128, 128)
    dat_order = 'C'
    
    # ========== æ•°æ®è·¯å¾„ ==========
    train_data = root / 'train' / 'seis'
    train_label = root / 'train' / 'fault'
    val_data = root / 'prediction' / 'seis'
    val_label = root / 'prediction' / 'fault'
    
    print('=' * 70)
    print('è®­ç»ƒé…ç½®')
    print('=' * 70)
    print(f'æ€»Epochs: {epochs}')
    print(f'å½’ä¸€åŒ–æ–¹æ³•: {"é²æ£’Z-score(MAD)" if use_robust_normalization else "ä¼ ç»ŸZ-score"}')
    print(f'å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}')
    print(f'æŸå¤±æƒé‡: BCE=0.3, Dice=0.7')
    print(f'ä¼˜åŒ–å™¨: {optimizer_type}, LR: {lr}, Weight Decay: {weight_decay}')
    print(f'è°ƒåº¦å™¨ç±»å‹: {scheduler_type}')
    print(f'æ¢¯åº¦è£å‰ª: {max_grad_norm}')
    print(f'è®¾å¤‡: {device}')
    print(f'æ¨¡å‹: {MODEL_CONFIG["model_name"]}')
    print(f'æ‰¹å¤§å°: {batch_size}, å·¥ä½œçº¿ç¨‹: {workers}')
    print('=' * 70)
    
    # ========== æ•°æ®è·¯å¾„æ£€æŸ¥ ==========
    print('æ£€æŸ¥æ•°æ®è·¯å¾„...')
    if not train_data.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {train_data}")
    if not train_label.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ ‡ç­¾è·¯å¾„ä¸å­˜åœ¨: {train_label}")
    if not val_data.exists():
        print(f"âš ï¸  è­¦å‘Š: éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {val_data}")
        print(f"âš ï¸  æ³¨æ„: å°†æ— æ³•è¿›è¡ŒéªŒè¯ï¼Œè®­ç»ƒå°†ç»§ç»­ä½†æ— æ³•æ£€æµ‹è¿‡æ‹Ÿåˆ")
        # è¿™é‡Œå¯ä»¥é€‰æ‹©åˆ›å»ºç©ºéªŒè¯é›†æˆ–è°ƒæ•´é€»è¾‘
    if not val_label.exists():
        print(f"âš ï¸  è­¦å‘Š: éªŒè¯æ ‡ç­¾è·¯å¾„ä¸å­˜åœ¨: {val_label}")
    
    # ========== æ•°æ®åŠ è½½ ==========
    print('åŠ è½½æ•°æ®...')
    ds_train = VolumeDataset(str(train_data), str(train_label), 
                            dat_dtype=dat_dtype, dat_shape=dat_shape, dat_order=dat_order)
    
    # æ£€æŸ¥éªŒè¯é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
    if val_data.exists() and val_label.exists():
        ds_val = VolumeDataset(str(val_data), str(val_label), 
                              dat_dtype=dat_dtype, dat_shape=dat_shape, dat_order=dat_order)
    else:
        # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼ˆä¸æ¨èä½†å¯ä»¥ç»§ç»­è®­ç»ƒï¼‰
        print("âš ï¸  ä½¿ç”¨è®­ç»ƒé›†çš„å‰10%ä½œä¸ºéªŒè¯é›†ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä½¿ç”¨æ—¶è¯·æä¾›éªŒè¯é›†ï¼‰")
        from torch.utils.data import random_split
        train_size = int(0.9 * len(ds_train))
        val_size = len(ds_train) - train_size
        ds_train, ds_val = random_split(ds_train, [train_size, val_size], 
                                       generator=torch.Generator().manual_seed(42))
    
    loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, 
                             num_workers=workers, pin_memory=True, 
                             persistent_workers=True if workers > 0 else False)
    loader_val = DataLoader(ds_val, batch_size=1, shuffle=False, 
                           num_workers=min(2, workers), pin_memory=False)
    
    print(f'è®­ç»ƒé›†å¤§å°: {len(ds_train)}')
    print(f'éªŒè¯é›†å¤§å°: {len(ds_val)}')
    
    # ========== å½’ä¸€åŒ–æ•ˆæœåˆ†æ ==========
    analyze_normalization_effect(
        loader_train, 
        use_robust_norm=use_robust_normalization,
        num_batches=3
    )
    
    # ========== æ¨¡å‹åˆå§‹åŒ– ==========
    device = torch.device(device)
    model = build_model_from_config(MODEL_CONFIG)
    model.to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}')
    print(f'å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}')
    
    # ========== ä¼˜åŒ–å™¨åˆå§‹åŒ– ==========
    opt = get_optimizer(
        model, 
        lr=lr, 
        optimizer_type=optimizer_type,
        weight_decay=weight_decay
    )
    
    # ========== æŸå¤±å‡½æ•° ==========
    criterion_bce = nn.BCEWithLogitsLoss()
    criterion_dice = DiceLoss()
    
    # ========== è‡ªé€‚åº”è°ƒåº¦å™¨åˆå§‹åŒ– ==========
    scheduler, plateau_scheduler = get_adaptive_scheduler(
        opt, 
        warmup_epochs=warmup_epochs, 
        total_epochs=epochs,
        scheduler_type=scheduler_type
    )
    
    # ========== è¿‡æ‹Ÿåˆæ£€æµ‹å™¨åˆå§‹åŒ– ==========
    overfit_detector = OverfittingDetector(
        patience=overfit_patience,
        min_delta=overfit_min_delta,
        gap_threshold=overfit_gap_threshold
    )
    
    # ========== ä¿å­˜ç›®å½•è®¾ç½® ==========
    checkpoints_root = Path('checkpoints1')

    # ä¿®æ”¹è¿™é‡Œï¼šç›´æ¥ä½¿ç”¨ model_name ä½œä¸º model_tag
    model_tag = MODEL_CONFIG["model_name"]  # ç®€åŒ–é€»è¾‘

    # å¦‚æœ model_tag å·²ç»åŒ…å« base_channels ä¿¡æ¯ï¼Œå°±ä¸é‡å¤æ·»åŠ 
    base_ch = MODEL_CONFIG.get('base_channels')
    if base_ch and f"_c{base_ch}" not in model_tag:
        model_tag = f"{model_tag}_c{base_ch}"
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    model_save_dir = checkpoints_root / model_tag / timestamp
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–¹ä¾¿å¿«é€Ÿè®¿é—®æœ€è¿‘ä¸€æ¬¡è¿è¡Œï¼šchecks/.../<model_tag>/latest/
    latest_dir = checkpoints_root / model_tag / 'latest'
    latest_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_save_path = model_save_dir / 'training_config.txt'
    with open(config_save_path, 'w') as f:
        f.write(f"è®­ç»ƒé…ç½®\n")
        f.write(f"========\n")
        f.write(f"Epochs: {epochs}\n")
        f.write(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
        f.write(f"å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}\n")
        f.write(f"æŸå¤±æƒé‡: BCE=0.3, Dice=0.7\n")
        f.write(f"ä¼˜åŒ–å™¨: {optimizer_type}\n")
        f.write(f"å­¦ä¹ ç‡: {lr}\n")
        f.write(f"æƒé‡è¡°å‡: {weight_decay}\n")
        f.write(f"è°ƒåº¦å™¨ç±»å‹: {scheduler_type}\n")
        f.write(f"æ‰¹å¤§å°: {batch_size}\n")
        f.write(f"å·¥ä½œçº¿ç¨‹: {workers}\n")
        f.write(f"AMPæ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}\n")
        f.write(f"æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
        f.write(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
    
    # ========== AMP scaler ==========
    if use_amp and device.type != 'cuda':
        print("âš ï¸  è­¦å‘Š: AMPä»…åœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨ï¼Œå·²ç¦ç”¨AMP")
        use_amp = False
    scaler = torch.cuda.amp.GradScaler() if (use_amp and device.type == 'cuda') else None
    
    # ========== è®°å½•å˜é‡ ==========
    best_val_loss = float('inf')
    best_val_iou = 0.0
    train_losses_history = []
    val_losses_history = []
    train_ious_history = []
    val_ious_history = []
    learning_rates_history = []
    
    # ========== è®­ç»ƒå¾ªç¯ ==========
    print('\nå¼€å§‹è®­ç»ƒ...')
    for epoch in range(1, epochs + 1):
        print(f'\n{"="*60}')
        print(f'Epoch {epoch}/{epochs}')
        print(f'{"="*60}')
        
        # è®­ç»ƒ
        train_metrics = train_epoch(
            model, loader_train, opt, criterion_bce, criterion_dice, 
            device, scaler=scaler, accum_steps=accum_steps, 
            max_grad_norm=max_grad_norm,
            use_robust_norm=use_robust_normalization
        )
        train_loss = train_metrics['total_loss']
        
        # éªŒè¯
        val_metrics = validate(
            model, loader_val, criterion_bce, criterion_dice, device,
            use_robust_norm=use_robust_normalization
        )
        val_loss = val_metrics['total_loss']
        
        # è®°å½•å†å²
        train_losses_history.append(train_loss)
        val_losses_history.append(val_loss)
        train_ious_history.append(train_metrics['iou'])
        val_ious_history.append(val_metrics['iou'])
        learning_rates_history.append(opt.param_groups[0]['lr'])
        
        # æ‰“å°ç»“æœ
        print(f"  Train Loss: {train_loss:.6f} (BCE: {train_metrics['bce_loss']:.6f}, "
              f"Dice: {train_metrics['dice_loss']:.6f}, IoU: {train_metrics['iou']:.6f})")
        print(f"  Val Loss:   {val_loss:.6f} (BCE: {val_metrics['bce_loss']:.6f}, "
              f"Dice: {val_metrics['dice_loss']:.6f}, IoU: {val_metrics['iou']:.6f})")
        print(f"  å½“å‰å­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}")
        
        # ========== æ›´æ–°è¿‡æ‹Ÿåˆæ£€æµ‹å™¨ ==========
        improved, overfitting_signals = overfit_detector.update(epoch, train_loss, val_loss)
        
        # æ˜¾ç¤ºè¿‡æ‹Ÿåˆæ£€æµ‹å™¨çŠ¶æ€
        print(f"\n  {overfit_detector.get_summary()}")
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
        should_stop, stop_reasons = overfit_detector.should_stop(epoch)
        if overfitting_signals:
            print(f"  âš ï¸  è¿‡æ‹Ÿåˆä¿¡å·:")
            for signal in overfitting_signals:
                print(f"     - {signal}")
        
        # ========== å­¦ä¹ ç‡è°ƒåº¦ ==========
        try:
            if scheduler_type == 'plateau':
                # ReduceLROnPlateauéœ€è¦ä¼ å…¥éªŒè¯æŸå¤±
                scheduler.step(val_loss)
            else:
                # å…¶ä»–è°ƒåº¦å™¨ç›´æ¥step
                scheduler.step()
        except Exception as e:
            print(f"  è°ƒåº¦å™¨stepå¼‚å¸¸: {e}")
            # ç®€å•å›é€€ï¼šæ‰‹åŠ¨é™ä½å­¦ä¹ ç‡
            if epoch % 20 == 0:
                for param_group in opt.param_groups:
                    param_group['lr'] *= 0.5
                print(f"  æ‰‹åŠ¨é™ä½å­¦ä¹ ç‡è‡³: {opt.param_groups[0]['lr']:.2e}")
        
        # ========== ä¿å­˜æ¨¡å‹ ==========
        # å‡†å¤‡è°ƒåº¦å™¨çŠ¶æ€
        scheduler_state = None
        plateau_scheduler_state = None
        
        if scheduler_type != 'plateau':
            scheduler_state = scheduler.state_dict()
        if plateau_scheduler is not None:
            plateau_scheduler_state = plateau_scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        last_path = model_save_dir / 'model_last.pth'
        torch.save({
            'epoch': epoch,
            'model_state': model.state_dict(),
            'opt_state': opt.state_dict(),
            'scheduler_state': scheduler_state,
            'plateau_scheduler_state': plateau_scheduler_state,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_iou': train_metrics['iou'],
            'val_iou': val_metrics['iou'],
            'train_history': train_losses_history,
            'val_history': val_losses_history,
            'train_ious': train_ious_history,
            'val_ious': val_ious_history,
            'lr_history': learning_rates_history,
            'model_config': MODEL_CONFIG,
            'normalization_method': 'robust' if use_robust_normalization else 'traditional'
        }, str(last_path))
        
        # ä¹Ÿå†™ä¸€ä»½åˆ° latest/
        torch.save({
            'epoch': epoch,
            'model_state': model.state_dict(),
            'opt_state': opt.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_iou': train_metrics['iou'],
            'val_iou': val_metrics['iou'],
            'model_config': MODEL_CONFIG,
            'normalization_method': 'robust' if use_robust_normalization else 'traditional'
        }, str(latest_dir / 'model_last.pth'))
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
        save_best_loss = False
        save_best_iou = False
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_best_loss = True
        
        if val_metrics['iou'] > best_val_iou:
            best_val_iou = val_metrics['iou']
            save_best_iou = True
        
        if save_best_loss:
            best_path = model_save_dir / 'model_best_loss.pth'
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'train_history': train_losses_history,
                'val_history': val_losses_history,
                'train_ious': train_ious_history,
                'val_ious': val_ious_history,
                'lr_history': learning_rates_history,
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(best_path))
            
            # æ›´æ–° latest ä¸­çš„ best_loss
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(latest_dir / 'model_best_loss.pth'))
            
            print(f'  âœ… ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹ (val_loss: {val_loss:.6f}, val_iou: {val_metrics["iou"]:.6f})')
        
        if save_best_iou:
            best_iou_path = model_save_dir / 'model_best_iou.pth'
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'train_history': train_losses_history,
                'val_history': val_losses_history,
                'train_ious': train_ious_history,
                'val_ious': val_ious_history,
                'lr_history': learning_rates_history,
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(best_iou_path))
            
            # æ›´æ–° latest ä¸­çš„ best_iou
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(latest_dir / 'model_best_iou.pth'))
            
            print(f'  âœ… ä¿å­˜æœ€ä½³IoUæ¨¡å‹ (val_iou: {val_metrics["iou"]:.6f}, val_loss: {val_loss:.6f})')
        
        # ========== å®šæœŸä¿å­˜è®­ç»ƒæ›²çº¿ ==========
        if epoch % 5 == 0 or epoch == epochs or should_stop:
            # æ‰©å±•çš„ç»˜å›¾å‡½æ•°
            def plot_extended_training_curves(train_losses, val_losses, train_ious, val_ious, 
                                            learning_rates=None, save_path=None):
                """ç»˜åˆ¶æ‰©å±•çš„è®­ç»ƒæ›²çº¿ï¼ŒåŒ…æ‹¬æŸå¤±å’ŒIoU"""
                if learning_rates is None:
                    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                else:
                    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
                
                epochs_range = range(1, len(train_losses) + 1)
                
                # æŸå¤±æ›²çº¿
                ax = axes[0, 0]
                ax.plot(epochs_range, train_losses, 'b-', label='Train Loss', alpha=0.7, linewidth=2)
                ax.plot(epochs_range, val_losses, 'r-', label='Val Loss', alpha=0.7, linewidth=2)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.set_title('Training and Validation Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # IoUæ›²çº¿
                ax = axes[0, 1]
                ax.plot(epochs_range, train_ious, 'g-', label='Train IoU', alpha=0.7, linewidth=2)
                ax.plot(epochs_range, val_ious, 'm-', label='Val IoU', alpha=0.7, linewidth=2)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('IoU')
                ax.set_title('Training and Validation IoU')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # æŸå¤±å·®è·æ›²çº¿
                ax = axes[1, 0]
                if len(train_losses) > 0 and len(val_losses) > 0:
                    gaps = [abs(t - v) for t, v in zip(train_losses, val_losses)]
                    ax.plot(epochs_range, gaps, 'c-', label='Train-Val Gap', alpha=0.7, linewidth=2)
                    ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Warning Threshold')
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('Gap')
                    ax.set_title('Train-Validation Gap')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                # IoUå·®è·æ›²çº¿
                ax = axes[1, 1]
                if len(train_ious) > 0 and len(val_ious) > 0:
                    iou_gaps = [abs(t - v) for t, v in zip(train_ious, val_ious)]
                    ax.plot(epochs_range, iou_gaps, 'y-', label='IoU Gap', alpha=0.7, linewidth=2)
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('IoU Gap')
                    ax.set_title('Train-Validation IoU Gap')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                # å­¦ä¹ ç‡æ›²çº¿
                if learning_rates is not None:
                    ax = axes[0, 2] if len(axes.shape) == 2 else axes[2]
                    ax.plot(epochs_range, learning_rates, color='purple', linestyle='-', 
                           label='Learning Rate', alpha=0.7, linewidth=2)
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('Learning Rate')
                    ax.set_title('Learning Rate Schedule')
                    ax.set_yscale('log')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                if save_path:
                    plt.savefig(save_path, dpi=150, bbox_inches='tight')
                plt.close()
            
            plot_path = model_save_dir / f'training_curve_epoch_{epoch}.png'
            plot_extended_training_curves(
                train_losses_history, val_losses_history, 
                train_ious_history, val_ious_history,
                learning_rates_history, plot_path
            )
            # å¦å­˜ä¸€ä»½åˆ° latest
            plot_extended_training_curves(
                train_losses_history, val_losses_history,
                train_ious_history, val_ious_history,
                learning_rates_history, 
                latest_dir / 'training_curve_latest.png'
            )
            print(f"  ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {plot_path}")
        
        # ========== æ¸…ç†ç¼“å­˜ ==========
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        # ========== æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ ==========
        if should_stop:
            print(f"\n{'!'*70}")
            print("æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œåœæ­¢è®­ç»ƒ!")
            print(f"åœæ­¢åŸå› :")
            for reason in stop_reasons:
                print(f"  - {reason}")
            print(f"æœ€ä½³æ¨¡å‹åœ¨ epoch {overfit_detector.best_epoch}, val_loss: {overfit_detector.best_val_loss:.6f}")
            print(f"{'!'*70}")
            
            # ä¿å­˜è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ
            overfit_info_path = model_save_dir / 'overfitting_report.txt'
            with open(overfit_info_path, 'w') as f:
                f.write("è¿‡æ‹Ÿåˆæ£€æµ‹æŠ¥å‘Š\n")
                f.write("="*50 + "\n")
                f.write(f"è®­ç»ƒåœæ­¢äº epoch {epoch}\n")
                f.write(f"æ€»è®­ç»ƒepochæ•°: {epochs}\n")
                f.write(f"å®é™…è®­ç»ƒepochæ•°: {epoch}\n")
                f.write(f"æœ€ä½³ epoch: {overfit_detector.best_epoch}\n")
                f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {overfit_detector.best_val_loss:.6f}\n")
                f.write(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}\n")
                f.write(f"åœæ­¢åŸå› :\n")
                for reason in stop_reasons:
                    f.write(f"  - {reason}\n")
                f.write("\nè®­ç»ƒå†å²:\n")
                f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss:.6f}\n")
                f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {val_loss:.6f}\n")
                f.write(f"  æœ€ç»ˆè®­ç»ƒIoU: {train_metrics['iou']:.6f}\n")
                f.write(f"  æœ€ç»ˆéªŒè¯IoU: {val_metrics['iou']:.6f}\n")
                f.write(f"  è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·: {abs(train_loss - val_loss):.6f}\n")
                f.write(f"  è®­ç»ƒ-éªŒè¯IoUå·®è·: {abs(train_metrics['iou'] - val_metrics['iou']):.6f}\n")
                f.write(f"  æœ€ç»ˆå­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}\n")
                f.write(f"  æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
                f.write(f"  æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
                f.write(f"  å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
            
            print(f"è¿‡æ‹ŸåˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {overfit_info_path}")
            
            # è®­ç»ƒæå‰åœæ­¢åï¼Œä¿å­˜æœ€ç»ˆè®­ç»ƒæ›²çº¿
            final_plot_path = model_save_dir / 'training_curve_final.png'
            plot_extended_training_curves(
                train_losses_history, val_losses_history,
                train_ious_history, val_ious_history,
                learning_rates_history, final_plot_path
            )
            break
    
    # ========== è®­ç»ƒå®Œæˆï¼ˆæœªæå‰åœæ­¢ï¼‰ ==========
    if not should_stop:
        print(f"\n{'='*70}")
        print("è®­ç»ƒå®Œæˆï¼Œæœªæ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        print(f"æœ€ä½³æ¨¡å‹åœ¨ epoch {overfit_detector.best_epoch}, val_loss: {overfit_detector.best_val_loss:.6f}")
        print(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}")
        print(f"{'='*70}")
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report_path = model_save_dir / 'training_report.txt'
        with open(report_path, 'w') as f:
            f.write("è®­ç»ƒæŠ¥å‘Š\n")
            f.write("="*50 + "\n")
            f.write(f"æ€»Epochs: {epochs}\n")
            f.write(f"å®ŒæˆEpochs: {epochs}\n")
            f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\n")
            f.write(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}\n")
            f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss:.6f}\n")
            f.write(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_loss:.6f}\n")
            f.write(f"æœ€ç»ˆè®­ç»ƒIoU: {train_metrics['iou']:.6f}\n")
            f.write(f"æœ€ç»ˆéªŒè¯IoU: {val_metrics['iou']:.6f}\n")
            f.write(f"è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·: {abs(train_loss - val_loss):.6f}\n")
            f.write(f"è®­ç»ƒ-éªŒè¯IoUå·®è·: {abs(train_metrics['iou'] - val_metrics['iou']):.6f}\n")
            f.write(f"ä¼˜åŒ–å™¨: {optimizer_type}\n")
            f.write(f"è°ƒåº¦å™¨: {scheduler_type}\n")
            f.write(f"åˆå§‹å­¦ä¹ ç‡: {lr}\n")
            f.write(f"æœ€ç»ˆå­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}\n")
            f.write(f"æƒé‡è¡°å‡: {weight_decay}\n")
            f.write(f"æ¢¯åº¦è£å‰ª: {max_grad_norm}\n")
            f.write(f"æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
            f.write(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
            f.write(f"AMPæ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}\n")
            f.write(f"æ‰¹å¤§å°: {batch_size}\n")
            f.write(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {accum_steps}\n")
            f.write(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
            f.write(f"å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}\n")
            f.write(f"æŸå¤±æƒé‡: BCE=0.3, Dice=0.7\n")
            f.write(f"å·¥ä½œçº¿ç¨‹: {workers}\n")
        
        print(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

if __name__ == '__main__':
    main()