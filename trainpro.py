import os
# å…è®¸é‡å¤çš„ OpenMPè¿è¡Œæ—¶ï¼ˆä¸æ¨èä½œä¸ºé•¿æœŸè§£å†³æ–¹æ¡ˆï¼Œåªæ˜¯ä¸´æ—¶å˜é€šï¼‰
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau
from datetime import datetime
import random

from models.unet3d import UNet3D
from dataloader import VolumeDataset
from models.AERB3d import AERBUNet3DLight
from models.attention_unet3d import LightAttentionUNet3D
from models.seunet3d import SEUNet3D
from models.AERB3d import AERBUNet3D
from models.attention_unet3d import AttentionUNet3D
from models.CBAM import CBAM_UNet3D


# ======= TF32 è®¾ç½®ï¼ˆRTX 3090 å¼ºçƒˆæ¨èï¼‰=======
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
# ============================================

class DiceFocalLoss(nn.Module):
    """
    ã€å…¨èƒ½å‹æŸå¤±å‡½æ•°ã€‘
    Dice Loss (å…³æ³¨å…¨å±€/å½¢çŠ¶/é‡å åº¦) + Focal Loss (å…³æ³¨å±€éƒ¨/éš¾åˆ†åƒç´ /è¾¹ç¼˜)
    """
    def __init__(self, weight_dice=1.0, weight_focal=1.0, gamma=2.0, alpha=0.25):
        super(DiceFocalLoss, self).__init__()
        self.weight_dice = weight_dice
        self.weight_focal = weight_focal
        self.gamma = gamma       # èšç„¦å‚æ•°ï¼šè¶Šå¤§è¶Šå…³æ³¨éš¾åˆ†æ ·æœ¬ï¼ˆå¦‚æ–­å±‚è¾¹ç¼˜ï¼‰
        self.alpha = alpha       # ç±»åˆ«å¹³è¡¡å‚æ•°ï¼šç”¨äºè°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æƒé‡åŸºç¡€å€¼
        self.eps = 1e-6

    def forward(self, logits, targets):
        """
        è¾“å…¥:
            logits: [B, 1, D, H, W] (æ¨¡å‹è¾“å‡ºï¼Œæœªç»è¿‡ Sigmoid)
            targets: [B, 1, D, H, W] (æ ‡ç­¾ï¼Œ0æˆ–1)
        """
        # 0. ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if targets.dtype != logits.dtype:
            targets = targets.type_as(logits)
            
        # --------------------------------------------
        # 1. è®¡ç®— Dice Loss (å…³æ³¨æ•´ä½“é‡å åº¦)
        # --------------------------------------------
        probs = torch.sigmoid(logits)
        
        # å±•å¹³è®¡ç®—ï¼Œé˜²æ­¢ Batch é—´å¹²æ‰°
        batch_size = logits.shape[0]
        probs_flat = probs.reshape(batch_size, -1)
        targets_flat = targets.reshape(batch_size, -1)
        
        intersection = (probs_flat * targets_flat).sum(dim=1)
        denominator = probs_flat.sum(dim=1) + targets_flat.sum(dim=1)
        
        dice_score = (2. * intersection + self.eps) / (denominator + self.eps)
        dice_loss = 1 - dice_score.mean()

        # --------------------------------------------
        # 2. è®¡ç®— Focal Loss (å…³æ³¨éš¾åˆ†æ ·æœ¬)
        # --------------------------------------------
        # è®¡ç®—æ¯ä¸ªåƒç´ çš„ BCE Loss (ä¸è¿›è¡Œå½’çº¦ reduction='none')
        # bce_loss = - [y * log(p) + (1-y) * log(1-p)]
        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        
        # [ä¼˜åŒ–] æ˜ç¡®æ­£è´Ÿæ ·æœ¬æƒé‡é€»è¾‘
        # alpha_t: target=1æ—¶ä¸ºalpha(0.75), target=0æ—¶ä¸º1-alpha(0.25)
        alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        
        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss
        return self.weight_dice * dice_loss + self.weight_focal * focal_loss.mean() 

# ============================================================================
# è®¾ç½®éšæœºç§å­
# ============================================================================
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

# è®¾ç½®éšæœºç§å­
set_seed(42)

# ============================================================================
# å¯ç¼–è¾‘æ¨¡å‹é…ç½®ï¼ˆåœ¨æ­¤å¤„ä¿®æ”¹ä»¥è®­ç»ƒä¸åŒæ¨¡å‹ï¼‰
# ============================================================================
MODEL_CONFIG = {
    "model_name": "attention_unet3d",  # å¯é€‰: 'unet3d', 'aerb_light', 'attn_light', 'seunet3d', 'aerb3d', 'attention_unet3d'
    "in_channels": 1,
    "out_channels": 1,
    "pretrained_ckpt": None,
}

# ç®€å•æ¨¡å‹æ³¨å†Œè¡¨ä¸æ„é€ å™¨
_MODEL_REGISTRY = {
    "unet3d": UNet3D,
    "aerb_light": AERBUNet3DLight,
    "attn_light": LightAttentionUNet3D,
    "seunet3d": SEUNet3D,
    'aerb3d': AERBUNet3D,
    'attention_unet3d': AttentionUNet3D,
    'cbam_unet3d': CBAM_UNet3D,
}

def augment_batch_data(x, y):
    """
    ã€å›é€€ç‰ˆã€‘ä»…ä¿ç•™æ°´å¹³/å‚ç›´ç¿»è½¬ã€‚
    å»æ‰äº† Transposeï¼Œå› ä¸ºå®é™…æµ‹è¯•è¯æ˜åœ°éœ‡æ•°æ®çš„æ–¹å‘æ€§å·®å¼‚è¿‡å¤§ï¼Œä¸é€‚åˆå¼ºåˆ¶äº¤æ¢ã€‚
    """
    # 1. éšæœºæ°´å¹³ç¿»è½¬ (Xè½´é•œåƒ)
    if random.random() > 0.5:
        x = torch.flip(x, dims=[4]) 
        y = torch.flip(y, dims=[4])
        
    # 2. éšæœºå‚ç›´ç¿»è½¬ (Yè½´é•œåƒ)
    if random.random() > 0.5:
        x = torch.flip(x, dims=[3])
        y = torch.flip(y, dims=[3])

    
    # 3. [æ–°å¢] éšæœº 90åº¦ æ—‹è½¬ (åœ¨ X-Y å¹³é¢)
    # åœ°éœ‡åˆ‡ç‰‡é€šå¸¸åœ¨æ°´å¹³æ–¹å‘ä¸Šæ²¡æœ‰ç»å¯¹çš„æ–¹å‘æ€§
    k = random.randint(0, 3)
    if k > 0:
        x = torch.rot90(x, k, dims=[3, 4])
        y = torch.rot90(y, k, dims=[3, 4])    

    return x, y

# ============================================================================
# ä¼˜åŒ–å™¨å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
# ============================================================================
def get_optimizer(model, lr=1e-4, optimizer_type='adamw', weight_decay=1e-4):
    """
    è·å–é€‚åˆ3Dåˆ†å‰²ç½‘ç»œçš„ä¼˜åŒ–å™¨
    
    Args:
        model: è¦ä¼˜åŒ–çš„æ¨¡å‹
        lr: åˆå§‹å­¦ä¹ ç‡
        optimizer_type: 'adamw', 'adam', 'sgd'
        weight_decay: L2æ­£åˆ™åŒ–å¼ºåº¦
    """
    if optimizer_type.lower() == 'adamw':
        # AdamW: æ›´å¥½çš„æƒé‡è¡°å‡å¤„ç†ï¼Œé€šå¸¸æ€§èƒ½æ›´å¥½
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            betas=(0.9, 0.999),      # åŠ¨é‡å‚æ•°
            eps=1e-8,                # æ•°å€¼ç¨³å®šæ€§
            weight_decay=weight_decay,  # L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            amsgrad=False            # é€šå¸¸ä¸éœ€è¦AMSGrad
        )
    elif optimizer_type.lower() == 'adam':
        # æ”¹è¿›çš„Adam
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=weight_decay  # æ·»åŠ æƒé‡è¡°å‡
        )
    elif optimizer_type.lower() == 'sgd':
        # SGD with momentum (å¯¹æŸäº›ä»»åŠ¡æœ‰æ•ˆ)
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=lr,
            momentum=0.9,           # åŠ¨é‡
            weight_decay=weight_decay,  # æƒé‡è¡°å‡
            nesterov=True           # NesterovåŠ¨é‡
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
    
    return optimizer


def build_model_from_config(cfg):
    """
    æ ¹æ® MODEL_CONFIG æ„é€ æ¨¡å‹å®ä¾‹å¹¶ï¼ˆå¯é€‰ï¼‰åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚
    ç›´æ¥ç¼–è¾‘ä¸Šæ–¹ MODEL_CONFIG å³å¯åˆ‡æ¢æ¨¡å‹ä¸å‚æ•°ã€‚
    """
    name = cfg.get("model_name", "unet3d")
    cls = _MODEL_REGISTRY.get(name)
    if cls is None:
        raise ValueError(f"Unknown model_name '{name}'. Available: {list(_MODEL_REGISTRY.keys())}")

    # ä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°å®ä¾‹åŒ–
    kwargs = {}
    if "in_channels" in cfg:
        kwargs["in_channels"] = cfg["in_channels"]
    if "out_channels" in cfg:
        kwargs["out_channels"] = cfg["out_channels"]

    # å®ä¾‹åŒ–
    model = cls(**{k: v for k, v in kwargs.items() if k in cls.__init__.__code__.co_varnames})

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆè‹¥æä¾›è·¯å¾„ï¼‰
    ckpt = cfg.get("pretrained_ckpt")
    if ckpt:
        ckpt_path = Path(ckpt)
        if ckpt_path.exists():
            sd = torch.load(str(ckpt_path), map_location="cpu")
            # æ”¯æŒå¸¸è§åŒ…è£…å­—æ®µ
            if isinstance(sd, dict) and "model_state" in sd:
                sd = sd["model_state"]
            if isinstance(sd, dict) and "state_dict" in sd:
                sd = sd["state_dict"]
            # å»æ‰ module. å‰ç¼€ï¼ˆDataParallelï¼‰
            new_sd = {}
            for k, v in sd.items():
                nk = k[len("module."):] if k.startswith("module.") else k
                new_sd[nk] = v
            try:
                model.load_state_dict(new_sd, strict=False)
                print(f"Loaded pretrained weights from {ckpt_path} (strict=False)")
            except Exception as e:
                print(f"Warning: failed to fully load pretrained weights: {e}")
        else:
            print(f"Warning: pretrained_ckpt path does not exist: {ckpt_path}")

    return model

# ============================================================================
# æŸå¤±å‡½æ•°
# ============================================================================
class DiceLoss(nn.Module):
    def __init__(self, eps=1e-6):
        super(DiceLoss, self).__init__()
        self.eps = eps

    def forward(self, logits, targets):
        probs = torch.sigmoid(logits)
        num = 2 * (probs * targets).sum(dim=(1, 2, 3, 4))
        den = probs.sum(dim=(1, 2, 3, 4)) + targets.sum(dim=(1, 2, 3, 4))
        dice = (num + self.eps) / (den + self.eps)
        return 1 - dice.mean()

# ============================================================================
# è¿‡æ‹Ÿåˆæ£€æµ‹å™¨
# ============================================================================
class OverfittingDetector:
    """
    æ£€æµ‹è¿‡æ‹Ÿåˆçš„ç±»
    æ£€æµ‹æ ‡å‡†ï¼š
    1. éªŒè¯æŸå¤±è¿ç»­ä¸Šå‡ï¼ˆç›¸å¯¹äºæœ€ä½ç‚¹ï¼‰
    2. è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å·®è·è¿‡å¤§
    3. Diceç³»æ•°ç­‰æŒ‡æ ‡å‡ºç°åˆ†æ­§
    """
    def __init__(self, patience=10, min_delta=1e-4, gap_threshold=0.5, history_window=5):
        """
        Args:
            patience: å®¹å¿éªŒè¯æŸå¤±ä¸ä¸‹é™çš„epochæ•°
            min_delta: éªŒè¯æŸå¤±çš„æœ€å°æ”¹å–„é˜ˆå€¼
            gap_threshold: è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å·®è·çš„é˜ˆå€¼
            history_window: ç”¨äºè®¡ç®—è¶‹åŠ¿çš„çª—å£å¤§å°
        """
        self.patience = patience
        self.min_delta = min_delta
        self.gap_threshold = gap_threshold
        self.history_window = history_window
        
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.counter = 0
        
        # è®°å½•å†å²
        self.train_losses = []
        self.val_losses = []
        self.train_val_gaps = []
        
    def update(self, epoch, train_loss, val_loss):
        """æ›´æ–°æ£€æµ‹å™¨çŠ¶æ€"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        gap = abs(train_loss - val_loss)
        self.train_val_gaps.append(gap)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³éªŒè¯æŸå¤±
        if val_loss < self.best_val_loss - self.min_delta:
            self.best_val_loss = val_loss
            self.best_epoch = epoch
            self.counter = 0
            improved = True
        else:
            self.counter += 1
            improved = False
            
        return improved, self.check_overfitting(epoch)
    
    def check_overfitting(self, epoch):
        """æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ"""
        overfitting_signals = []
        
        # 1. æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦è¿ç»­ä¸Šå‡
        if len(self.val_losses) >= self.history_window:
            recent_val = self.val_losses[-self.history_window:]
            val_trend = np.polyfit(range(len(recent_val)), recent_val, 1)[0]
            if val_trend > 0.01:  # æ­£æ–œç‡ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
                overfitting_signals.append(f"éªŒè¯æŸå¤±è¿ç»­ä¸Šå‡ (è¶‹åŠ¿: {val_trend:.4f})")
        
        # 2. æ£€æŸ¥è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±çš„å·®è·
        if len(self.train_val_gaps) > 0:
            recent_gap = self.train_val_gaps[-1]
            if recent_gap > self.gap_threshold:
                overfitting_signals.append(f"è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·è¿‡å¤§ ({recent_gap:.4f} > {self.gap_threshold})")
        
        # 3. æ£€æŸ¥è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸ä¸‹é™
        if len(self.train_losses) >= self.history_window:
            recent_train = self.train_losses[-self.history_window:]
            train_trend = np.polyfit(range(len(recent_train)), recent_train, 1)[0]
            if train_trend < -0.01 and self.counter >= self.patience // 2:
                overfitting_signals.append("è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ä½†éªŒè¯æŸå¤±åœæ»")
        
        return overfitting_signals
    
    def get_summary(self):
        """è·å–æ£€æµ‹å™¨çŠ¶æ€æ‘˜è¦"""
        if len(self.train_losses) == 0:
            return "æ— è®­ç»ƒå†å²"
        
        summary = [
            f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f} (epoch {self.best_epoch})",
            f"å½“å‰éªŒè¯æŸå¤±: {self.val_losses[-1]:.6f}",
            f"å½“å‰è®­ç»ƒæŸå¤±: {self.train_losses[-1]:.6f}",
            f"è®­ç»ƒ-éªŒè¯å·®è·: {self.train_val_gaps[-1]:.6f}",
            f"è¿ç»­æœªæ”¹å–„epochæ•°: {self.counter}/{self.patience}"
        ]
        
        # æ·»åŠ è¶‹åŠ¿ä¿¡æ¯
        if len(self.val_losses) >= 3:
            recent_val = self.val_losses[-3:]
            val_trend = np.polyfit(range(len(recent_val)), recent_val, 1)[0]
            trend_symbol = 'â†‘' if val_trend > 0.001 else ('â†“' if val_trend < -0.001 else 'â†’')
            summary.append(f"éªŒè¯æŸå¤±è¶‹åŠ¿: {trend_symbol} ({val_trend:.6f})")
        
        return "\n".join(summary)
    
    def should_stop(self, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        overfitting_signals = self.check_overfitting(epoch)
        # ä¸»è¦åœæ­¢æ¡ä»¶ï¼šéªŒè¯æŸå¤±è¿ç»­patienceæ¬¡æœªæ”¹å–„
        should_stop = (self.counter >= self.patience)
        return should_stop, overfitting_signals

# ============================================================================
# å¯è§†åŒ–å·¥å…·
# ============================================================================
def plot_training_curves(train_losses, val_losses, learning_rates=None, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    if learning_rates is None:
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    else:
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    ax = axes[0] if learning_rates is None else axes[0]
    ax.plot(epochs, train_losses, 'b-', label='Train Loss', alpha=0.7, linewidth=2)
    ax.plot(epochs, val_losses, 'r-', label='Val Loss', alpha=0.7, linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Training and Validation Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # å·®è·æ›²çº¿
    ax = axes[1] if learning_rates is None else axes[1]
    if len(train_losses) > 0 and len(val_losses) > 0:
        gaps = [abs(t - v) for t, v in zip(train_losses, val_losses)]
        ax.plot(epochs, gaps, 'g-', label='Train-Val Gap', alpha=0.7, linewidth=2)
        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Warning Threshold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Gap')
        ax.set_title('Train-Validation Gap')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    if learning_rates is not None:
        ax = axes[2]
        # ä½¿ç”¨æ˜¾å¼ color/linestyleï¼Œé¿å…åƒ 'purple-' è¿™æ ·çš„éæ³• format string
        ax.plot(epochs, learning_rates, color='purple', linestyle='-', label='Learning Rate', alpha=0.7, linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.set_title('Learning Rate Schedule')
        ax.set_yscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

# ============================================================================
# å½’ä¸€åŒ–å‡½æ•°ï¼ˆé²æ£’ç‰ˆæœ¬ï¼‰
# ============================================================================
def _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0), device=None):
    """
    GPUåŠ é€Ÿç‰ˆé²æ£’Z-scoreæ ‡å‡†åŒ–ï¼šä½¿ç”¨ä¸­ä½æ•°å’ŒMADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
    ç›´æ¥åœ¨GPUä¸Šæ‰§è¡Œï¼Œé¿å…CPU-GPUæ•°æ®å¾€è¿”
    
    Args:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, C, Z, Y, X) æˆ– (B, Z, Y, X)
        use_mad: æ˜¯å¦ä½¿ç”¨MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰ä»£æ›¿æ ‡å‡†å·®
        clip_range: æˆªæ–­èŒƒå›´
        device: è®¡ç®—è®¾å¤‡ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨xçš„è®¾å¤‡ï¼‰
    """
    # ç¡®ä¿xæ˜¯torchå¼ é‡ä¸”åœ¨GPUä¸Š
    if not isinstance(x, torch.Tensor):
        x = torch.from_numpy(np.asarray(x)).float()
    
    if device is None:
        device = x.device
    
    x = x.to(device)
    
    if x.ndim == 5:  # (B, C, Z, Y, X)
        spatial_axes = (2, 3, 4)
        n_samples = x.shape[0] * x.shape[1]
        x_reshaped = x.reshape(n_samples, -1)
    elif x.ndim == 4:  # (B, Z, Y, X)
        spatial_axes = (1, 2, 3)
        n_samples = x.shape[0]
        x_reshaped = x.reshape(n_samples, -1)
    else:
        raise ValueError('Unsupported input ndim for normalization: %d' % x.ndim)
    
    # GPUä¸Šè®¡ç®—ä¸­ä½æ•°
    center = torch.median(x_reshaped, dim=1, keepdim=True)[0]
    
    if use_mad:
        # è®¡ç®—MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
        abs_dev = torch.abs(x_reshaped - center)
        mad = torch.median(abs_dev, dim=1, keepdim=True)[0]
        # MAD -> std: Ïƒ â‰ˆ 1.4826 * MAD
        scale = torch.where(mad > 1e-6, mad * 1.4826, torch.ones_like(mad))
    else:
        # ä½¿ç”¨æ ‡å‡†å·®
        scale = torch.std(x_reshaped, dim=1, keepdim=True)
        scale = torch.where(scale > 1e-6, scale, torch.ones_like(scale))
    
    # å½’ä¸€åŒ–
    x_norm = (x_reshaped - center) / scale
    
    # æˆªæ–­å¼‚å¸¸å€¼
    if clip_range:
        x_norm = torch.clamp(x_norm, clip_range[0], clip_range[1])
    
    # æ¢å¤åŸå§‹å½¢çŠ¶
    if x.ndim == 5:
        x_norm = x_norm.reshape(x.shape)
    elif x.ndim == 4:
        x_norm = x_norm.reshape(x.shape)
    
    return x_norm.float()


def _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0), device=None):
    """
    GPUåŠ é€Ÿç‰ˆä¼ ç»ŸZ-scoreæ ‡å‡†åŒ–
    ç›´æ¥åœ¨GPUä¸Šæ‰§è¡Œï¼Œé¿å…CPU-GPUæ•°æ®å¾€è¿”
    
    Args:
        x: è¾“å…¥å¼ é‡
        clip_range: æˆªæ–­èŒƒå›´
        device: è®¡ç®—è®¾å¤‡
    """
    # ç¡®ä¿xæ˜¯torchå¼ é‡ä¸”åœ¨GPUä¸Š
    if not isinstance(x, torch.Tensor):
        x = torch.from_numpy(np.asarray(x)).float()
    
    if device is None:
        device = x.device
    
    x = x.to(device)
    
    if x.ndim == 5:  # (B, C, Z, Y, X)
        n_samples = x.shape[0] * x.shape[1]
        x_reshaped = x.reshape(n_samples, -1)
    elif x.ndim == 4:  # (B, Z, Y, X)
        n_samples = x.shape[0]
        x_reshaped = x.reshape(n_samples, -1)
    else:
        raise ValueError('Unsupported input ndim for normalization: %d' % x.ndim)
    
    # GPUä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean = torch.mean(x_reshaped, dim=1, keepdim=True)
    std = torch.std(x_reshaped, dim=1, keepdim=True)
    std = torch.where(std > 1e-6, std, torch.ones_like(std))
    
    # å½’ä¸€åŒ–
    x_norm = (x_reshaped - mean) / std
    
    # æˆªæ–­å¼‚å¸¸å€¼
    if clip_range:
        x_norm = torch.clamp(x_norm, clip_range[0], clip_range[1])
    
    # æ¢å¤åŸå§‹å½¢çŠ¶
    if x.ndim == 5:
        x_norm = x_norm.reshape(x.shape)
    elif x.ndim == 4:
        x_norm = x_norm.reshape(x.shape)
    
    return x_norm.float()


def _batch_iou(logits, targets, threshold=0.5, eps=1e-7):
    preds = (torch.sigmoid(logits) > threshold)
    targets_bool = targets.bool()
    intersection = (preds & targets_bool).sum(dim=(1, 2, 3, 4)).float()
    union = (preds | targets_bool).sum(dim=(1, 2, 3, 4)).float()
    iou = (intersection + eps) / (union + eps)
    return iou.mean()

# ============================================================================
# å½’ä¸€åŒ–æ•ˆæœåˆ†æå‡½æ•°
# ============================================================================
def analyze_normalization_effect(loader, use_robust_norm=True, num_batches=5):
    """
    åˆ†æå½’ä¸€åŒ–æ•ˆæœ
    """
    print("\n" + "="*60)
    print("å½’ä¸€åŒ–æ•ˆæœåˆ†æ")
    print("="*60)
    
    before_stats = {'mean': [], 'std': [], 'median': [], 'mad': [], 'min': [], 'max': []}
    after_stats = {'mean': [], 'std': [], 'median': [], 'mad': [], 'min': [], 'max': []}
    
    for i, (x, _) in enumerate(loader):
        if i >= num_batches:
            break
            
        # åŸå§‹ç»Ÿè®¡
        x_np = x.numpy()
        before_stats['mean'].append(np.mean(x_np))
        before_stats['std'].append(np.std(x_np))
        before_stats['median'].append(np.median(x_np))
        before_stats['mad'].append(np.median(np.abs(x_np - np.median(x_np))))
        before_stats['min'].append(np.min(x_np))
        before_stats['max'].append(np.max(x_np))
        
        # å½’ä¸€åŒ–åç»Ÿè®¡
        if use_robust_norm:
            x_norm = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).numpy()
        else:
            x_norm = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).numpy()
        
        after_stats['mean'].append(np.mean(x_norm))
        after_stats['std'].append(np.std(x_norm))
        after_stats['median'].append(np.median(x_norm))
        after_stats['mad'].append(np.median(np.abs(x_norm - np.median(x_norm))))
        after_stats['min'].append(np.min(x_norm))
        after_stats['max'].append(np.max(x_norm))
    
    print("\nå½’ä¸€åŒ–å‰åå¯¹æ¯”:")
    print("="*60)
    print("             å½’ä¸€åŒ–å‰                å½’ä¸€åŒ–å")
    print("="*60)
    
    for stat_name in ['mean', 'std', 'median', 'mad', 'min', 'max']:
        before_avg = np.mean(before_stats[stat_name])
        before_std = np.std(before_stats[stat_name])
        after_avg = np.mean(after_stats[stat_name])
        after_std = np.std(after_stats[stat_name])
        
        print(f"{stat_name:6s}: {before_avg:8.4f} Â± {before_std:.4f}  ->  {after_avg:8.4f} Â± {after_std:.4f}")
    
    print("="*60)
    print(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_norm else 'ä¼ ç»ŸZ-score'}")
    print(f"æˆªæ–­èŒƒå›´: {(-4.0, 4.0) if use_robust_norm else (-3.0, 3.0)}")
    
    return before_stats, after_stats

# ============================================================================
# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¸¦æ¢¯åº¦è£å‰ªï¼‰
# ============================================================================
def train_epoch(model, loader, opt, criterion,
                device, scaler=None, accum_steps=1, max_grad_norm=1.0,
                use_robust_norm=True,scheduler=None):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒ…å«æ¢¯åº¦è£å‰ª
    
    Args:
        max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
        use_robust_norm: æ˜¯å¦ä½¿ç”¨é²æ£’å½’ä¸€åŒ–
    """
    model.train()
    running_loss = 0.0
    running_bce = 0.0
    running_dice = 0.0
    running_iou = 0.0
    opt.zero_grad()
    
    for step, (x, y) in enumerate(tqdm(loader, desc='train', leave=False), start=1):
        # ===== ç›´æ¥è½¬åˆ°GPUï¼Œç§»é™¤å¢å¼ºæ­¥éª¤ =====
        x = x.to(device)
        y = y.float().to(device)
        
        x, y = augment_batch_data(x, y)
        
        # ===== GPUä¸Šæ‰§è¡Œå½’ä¸€åŒ– =====
        if use_robust_norm:
            x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0), device=device)
        else:
            x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0), device=device)

        # æ··åˆç²¾åº¦å‰å‘
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(x)
                loss = criterion(logits, y)  # ç›´æ¥è°ƒç”¨ DiceFocalLoss
                batch_iou = _batch_iou(logits, y)
            scaler.scale(loss / accum_steps).backward()
        else:
            logits = model(x)
            loss = criterion(logits, y)      # ç›´æ¥è°ƒç”¨ DiceFocalLoss
            batch_iou = _batch_iou(logits, y)
            (loss / accum_steps).backward()


        # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ accum_steps æ­¥æ›´æ–°ä¸€æ¬¡
        if (step % accum_steps) == 0:
            if scaler is not None:
                # æ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„æ¢¯åº¦è£å‰ª
                scaler.unscale_(opt)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(opt)
                scaler.update()
            else:
                # æ™®é€šè®­ç»ƒæ—¶çš„æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                opt.step()
            opt.zero_grad()
            if scheduler is not None:
                scheduler.step()
    
    # è‹¥æœ€åä¸è¶³ accum_stepsï¼Œä¹Ÿè¦æ‰§è¡Œä¸€æ¬¡ stepï¼ˆå®‰å…¨å¤„ç†ï¼‰
    if len(loader) % accum_steps != 0:
        if scaler is not None:
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(opt)
            scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            opt.step()
        opt.zero_grad()
    
    total = len(loader.dataset)
    return {
        'total_loss': running_loss / total,
        'bce_loss': running_bce / total,
        'dice_loss': running_dice / total,
        'iou': running_iou / total
    }

def validate(model, loader, criterion, device, use_robust_norm=True):
    """
    éªŒè¯å‡½æ•°ï¼ˆå·²ä¿®æ­£ï¼šä¸è®­ç»ƒLossä¿æŒä¸€è‡´ï¼‰
    """
    model.eval()
    running_loss = 0.0
    running_bce = 0.0
    running_dice = 0.0
    running_iou = 0.0
    
    # å®šä¹‰è¾…åŠ©è®¡ç®—å‡½æ•°ï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼Œä¸å½±å“åå‘ä¼ æ’­æˆ–Lossè®¡ç®—ï¼‰
    # è¿™é‡Œçš„ BCE ç”¨äºè§‚å¯Ÿçº¯ç²¹çš„åˆ†ç±»è¯¯å·®
    def calc_bce(logits, targets):
        return F.binary_cross_entropy_with_logits(logits, targets)
        
    # è¿™é‡Œçš„ Dice ç”¨äºè§‚å¯Ÿçº¯ç²¹çš„é‡å åº¦
    def calc_dice(logits, targets):
        probs = torch.sigmoid(logits)
        intersection = (probs * targets).sum()
        union = probs.sum() + targets.sum()
        dice = (2. * intersection + 1e-6) / (union + 1e-6)
        return 1 - dice

    with torch.no_grad():
        for x, y in tqdm(loader, desc='val', leave=False):
            # 1. å½’ä¸€åŒ–ï¼ˆä¿æŒä¸è®­ç»ƒä¸€è‡´ï¼‰
            if use_robust_norm:
                # æ³¨æ„ï¼šè¿™é‡Œä¹Ÿéœ€è¦åŠ ä¸Š device=device ä»¥é˜²ä¸‡ä¸€ï¼Œæˆ–è€…ä¾èµ– .to(device)
                x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0), device=device)
            else:
                x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0), device=device)
            
            x = x.to(device)
            y = y.float().to(device)
            
            logits = model(x)
            
            # ====================================================
            # 2. æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„ Loss å‡½æ•°
            # ====================================================
            # ä¹‹å‰æ˜¯æ‰‹åŠ¨åŠ æƒ 0.3*BCE + 0.7*Diceï¼Œè¿™æ˜¯é”™è¯¯çš„ã€‚
            # ç°åœ¨ç›´æ¥è°ƒç”¨ä¼ å…¥çš„ DiceFocalLoss
            loss = criterion(logits, y) 
            
            # ====================================================
            # 3. è¾…åŠ©æŒ‡æ ‡è®¡ç®— (ä»…ç”¨äºæ˜¾ç¤º/æ—¥å¿—)
            # ====================================================
            # ä¸ºäº†è®©æ—¥å¿—é‡Œçš„ bce_loss å’Œ dice_loss æœ‰æ•°æ®ï¼Œæˆ‘ä»¬å•ç‹¬ç®—ä¸€ä¸‹
            # æ³¨æ„ï¼šè¿™é‡Œçš„ loss_dice æ˜¯çº¯ Diceï¼Œè€Œ criterion é‡Œé¢å¯èƒ½åŒ…å« Focal
            val_bce = calc_bce(logits, y)
            val_dice = calc_dice(logits, y)
            
            batch_iou = _batch_iou(logits, y)
            
            # ç´¯åŠ 
            batch_size = x.size(0)
            running_loss += loss.item() * batch_size
            running_bce += val_bce.item() * batch_size
            running_dice += val_dice.item() * batch_size
            running_iou += batch_iou.item() * batch_size
    
    total = len(loader.dataset)
    return {
        'total_loss': running_loss / total, # è¿™æ˜¯æœ€é‡è¦çš„æŒ‡æ ‡ï¼Œç”¨äº Early Stopping
        'bce_loss': running_bce / total,    # ä»…ä½œå‚è€ƒ
        'dice_loss': running_dice / total,  # ä»…ä½œå‚è€ƒ
        'iou': running_iou / total          # é‡è¦çš„è¯„ä»·æŒ‡æ ‡
    }

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    # ========== è®­ç»ƒé…ç½® ==========
    root = Path('.')
    epochs = 200               # æ€»è®­ç»ƒepochæ•°ï¼ˆè‡ªé€‚åº”è°ƒåº¦å™¨ä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰
    batch_size = 8              # æ‰¹å¤§å°
    lr = 1e-4                    # åˆå§‹å­¦ä¹ ç‡
    workers = 8               # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ========== å½’ä¸€åŒ–é…ç½® ==========
    use_robust_normalization = True  # æ˜¯å¦ä½¿ç”¨é²æ£’å½’ä¸€åŒ–ï¼ˆæ¨èTrueï¼‰
    norm_clip_range = (-4.0, 4.0) if use_robust_normalization else (-3.0, 3.0)
    
    # ========== ä¼˜åŒ–å™¨é…ç½® ==========
    optimizer_type = 'adamw'        # å¯é€‰: 'adamw', 'adam', 'sgd'
    weight_decay = 1e-4             # L2æ­£åˆ™åŒ–å¼ºåº¦
    max_grad_norm = 1.0             # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    
    # ========== è°ƒåº¦å™¨é…ç½® ==========
    scheduler_type = 'one_cycle'         # 'auto'ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰ã€'cosine_warm'ã€'cosine'ã€'plateau'ã€'step'
    warmup_epochs = None            # Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
    
    # ========== è¿‡æ‹Ÿåˆæ£€æµ‹å‚æ•° ==========
    overfit_patience = 500          # å®¹å¿éªŒè¯æŸå¤±ä¸ä¸‹é™çš„epochæ•°
    overfit_min_delta = 1e-4        # æœ€å°æ”¹å–„é˜ˆå€¼
    overfit_gap_threshold = 0.5     # è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·é˜ˆå€¼
    
    # ========== å…¶ä»–è®­ç»ƒé…ç½® ==========
    use_amp = True                  # æ··åˆç²¾åº¦è®­ç»ƒ
    accum_steps = 1                 # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    
    # æ•°æ®æ ¼å¼é…ç½®
    dat_dtype = 'float32'
    dat_shape = (128, 128, 128)
    dat_order = 'C'
    
    # ========== æ•°æ®è·¯å¾„ ==========
    train_data = root / 'train' / 'seis'
    train_label = root / 'train' / 'fault'
    val_data = root / 'prediction' / 'seis'
    val_label = root / 'prediction' / 'fault'
    
    print('=' * 70)
    print('è®­ç»ƒé…ç½®')
    print('=' * 70)
    print(f'æ€»Epochs: {epochs}')
    print(f'å½’ä¸€åŒ–æ–¹æ³•: {"é²æ£’Z-score(MAD)" if use_robust_normalization else "ä¼ ç»ŸZ-score"}')
    print(f'å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}')
    print(f'æŸå¤±æƒé‡: BCE=0.3, Dice=0.7')
    print(f'ä¼˜åŒ–å™¨: {optimizer_type}, LR: {lr}, Weight Decay: {weight_decay}')
    print(f'è°ƒåº¦å™¨ç±»å‹: {scheduler_type}')
    print(f'æ¢¯åº¦è£å‰ª: {max_grad_norm}')
    print(f'è®¾å¤‡: {device}')
    print(f'æ¨¡å‹: {MODEL_CONFIG["model_name"]}')
    print(f'æ‰¹å¤§å°: {batch_size}, å·¥ä½œçº¿ç¨‹: {workers}')
    print('=' * 70)
    
    # ========== æ•°æ®è·¯å¾„æ£€æŸ¥ ==========
    print('æ£€æŸ¥æ•°æ®è·¯å¾„...')
    if not train_data.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {train_data}")
    if not train_label.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ ‡ç­¾è·¯å¾„ä¸å­˜åœ¨: {train_label}")
    if not val_data.exists():
        print(f"âš ï¸  è­¦å‘Š: éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {val_data}")
        print(f"âš ï¸  æ³¨æ„: å°†æ— æ³•è¿›è¡ŒéªŒè¯ï¼Œè®­ç»ƒå°†ç»§ç»­ä½†æ— æ³•æ£€æµ‹è¿‡æ‹Ÿåˆ")
        # è¿™é‡Œå¯ä»¥é€‰æ‹©åˆ›å»ºç©ºéªŒè¯é›†æˆ–è°ƒæ•´é€»è¾‘
    if not val_label.exists():
        print(f"âš ï¸  è­¦å‘Š: éªŒè¯æ ‡ç­¾è·¯å¾„ä¸å­˜åœ¨: {val_label}")
    
    # ========== æ•°æ®åŠ è½½ ==========
    print('åŠ è½½æ•°æ®...')
    ds_train = VolumeDataset(str(train_data), str(train_label), 
                            dat_dtype=dat_dtype, dat_shape=dat_shape, dat_order=dat_order)
    
    # æ£€æŸ¥éªŒè¯é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
    if val_data.exists() and val_label.exists():
        ds_val = VolumeDataset(str(val_data), str(val_label), 
                              dat_dtype=dat_dtype, dat_shape=dat_shape, dat_order=dat_order)
    else:
        # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼ˆä¸æ¨èä½†å¯ä»¥ç»§ç»­è®­ç»ƒï¼‰
        print("âš ï¸  ä½¿ç”¨è®­ç»ƒé›†çš„å‰10%ä½œä¸ºéªŒè¯é›†ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä½¿ç”¨æ—¶è¯·æä¾›éªŒè¯é›†ï¼‰")
        from torch.utils.data import random_split
        train_size = int(0.9 * len(ds_train))
        val_size = len(ds_train) - train_size
        ds_train, ds_val = random_split(ds_train, [train_size, val_size], 
                                       generator=torch.Generator().manual_seed(42))
    
    loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, 
                             num_workers=workers, pin_memory=True, 
                             persistent_workers=True if workers > 0 else False)
    loader_val = DataLoader(ds_val, batch_size=1, shuffle=False, 
                           num_workers=min(2, workers), pin_memory=False)
    
    print(f'è®­ç»ƒé›†å¤§å°: {len(ds_train)}')
    print(f'éªŒè¯é›†å¤§å°: {len(ds_val)}')
    
    # ========== å½’ä¸€åŒ–æ•ˆæœåˆ†æ ==========
    analyze_normalization_effect(
        loader_train, 
        use_robust_norm=use_robust_normalization,
        num_batches=3
    )
    
    # ========== æ¨¡å‹åˆå§‹åŒ– ==========
    device = torch.device(device)
    model = build_model_from_config(MODEL_CONFIG)
    model.to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}')
    print(f'å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}')
    
    # ========== ä¼˜åŒ–å™¨åˆå§‹åŒ– ==========
    opt = get_optimizer(
        model, 
        lr=lr, 
        optimizer_type=optimizer_type,
        weight_decay=weight_decay
    )
    
    # ========== æŸå¤±å‡½æ•° ==========
    # alpha=0.25 (æ ¹æ®åŸè®ºæ–‡ï¼Œé…åˆgamma=2ä½¿ç”¨)
    # gamma=2.0 (æ ‡å‡†èšç„¦å‚æ•°)
    # weight_dice=1.0, weight_focal=1.0 (ä¸¤è€…åŒç­‰é‡è¦ï¼Œæˆ–è€… dice=0.8, focal=1.2 å¼ºè°ƒè¾¹ç¼˜)
    criterion = DiceFocalLoss(weight_dice=1.0, weight_focal=1.0, gamma=2.0, alpha=0.75)
    criterion.to(device)
    
    # ========== è‡ªé€‚åº”è°ƒåº¦å™¨åˆå§‹åŒ– ==========
    steps_per_epoch = len(loader_train) 
    total_steps = steps_per_epoch * epochs

    print(f"One-Cycle é…ç½®: Max LR=1e-3, Total Steps={total_steps}")

    # 2. å®šä¹‰è°ƒåº¦å™¨
    # max_lr é€šå¸¸è®¾ä¸ºåˆå§‹ lr çš„ 10 å€å·¦å³ã€‚
    # 3D U-Net æ¯”è¾ƒè„†å¼±ï¼Œå¦‚æœä½ åŸæ¥ç”¨ 1e-4ï¼Œè¿™é‡Œ max_lr å»ºè®®ç»™ 1e-3 æˆ– 5e-4
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        opt,
        max_lr=1e-3,           # æœ€å¤§å­¦ä¹ ç‡å³°å€¼
        total_steps=total_steps,
        pct_start=0.3,         # å‰30%çš„æ—¶é—´ç”¨æ¥å‡æ¸©(Warmup)
        div_factor=25.0,       # åˆå§‹å­¦ä¹ ç‡ = max_lr / 25
        final_div_factor=1e4,  # æœ€ç»ˆå­¦ä¹ ç‡ = åˆå§‹å­¦ä¹ ç‡ / 10000
        anneal_strategy='cos'  # ä½¿ç”¨ä½™å¼¦å½¢çŠ¶
    )

    # æ³¨æ„ï¼šOneCycle ä¸éœ€è¦ plateau_schedulerï¼ŒæŠŠè¿™è¡Œåˆ æ‰æˆ–è®¾ä¸º None
    plateau_scheduler = None
    
    # ========== è¿‡æ‹Ÿåˆæ£€æµ‹å™¨åˆå§‹åŒ– ==========
    overfit_detector = OverfittingDetector(
        patience=overfit_patience,
        min_delta=overfit_min_delta,
        gap_threshold=overfit_gap_threshold
    )
    
    # ========== ä¿å­˜ç›®å½•è®¾ç½® ==========
    checkpoints_root = Path('checkpoints3')

    # ä¿®æ”¹è¿™é‡Œï¼šç›´æ¥ä½¿ç”¨ model_name ä½œä¸º model_tag
    model_tag = MODEL_CONFIG["model_name"]  # ç®€åŒ–é€»è¾‘
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    model_save_dir = checkpoints_root / model_tag / timestamp
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–¹ä¾¿å¿«é€Ÿè®¿é—®æœ€è¿‘ä¸€æ¬¡è¿è¡Œï¼šchecks/.../<model_tag>/latest/
    latest_dir = checkpoints_root / model_tag / 'latest'
    latest_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_save_path = model_save_dir / 'training_config.txt'
    with open(config_save_path, 'w') as f:
        f.write(f"è®­ç»ƒé…ç½®\n")
        f.write(f"========\n")
        f.write(f"Epochs: {epochs}\n")
        f.write(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
        f.write(f"å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}\n")
        f.write(f"æŸå¤±æƒé‡: BCE=0.3, Dice=0.7\n")
        f.write(f"ä¼˜åŒ–å™¨: {optimizer_type}\n")
        f.write(f"å­¦ä¹ ç‡: {lr}\n")
        f.write(f"æƒé‡è¡°å‡: {weight_decay}\n")
        f.write(f"è°ƒåº¦å™¨ç±»å‹: {scheduler_type}\n")
        f.write(f"æ‰¹å¤§å°: {batch_size}\n")
        f.write(f"å·¥ä½œçº¿ç¨‹: {workers}\n")
        f.write(f"AMPæ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}\n")
        f.write(f"æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
        f.write(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
    
    # ========== AMP scaler ==========
    if use_amp and device.type != 'cuda':
        print("âš ï¸  è­¦å‘Š: AMPä»…åœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨ï¼Œå·²ç¦ç”¨AMP")
        use_amp = False
    scaler = torch.cuda.amp.GradScaler() if (use_amp and device.type == 'cuda') else None
    
    # ========== è®°å½•å˜é‡ ==========
    best_val_loss = float('inf')
    best_val_iou = 0.0
    train_losses_history = []
    val_losses_history = []
    train_ious_history = []
    val_ious_history = []
    learning_rates_history = []
    
    # ========== è®­ç»ƒå¾ªç¯ ==========
    print('\nå¼€å§‹è®­ç»ƒ...')
    for epoch in range(1, epochs + 1):
        print(f'\n{"="*60}')
        print(f'Epoch {epoch}/{epochs}')
        print(f'{"="*60}')
        
        # è®­ç»ƒ
        train_metrics = train_epoch(
            model, loader_train, opt, criterion,
            device, scaler=scaler, accum_steps=accum_steps, 
            max_grad_norm=max_grad_norm,
            use_robust_norm=use_robust_normalization,scheduler=scheduler
        )
        train_loss = train_metrics['total_loss']
        
        # éªŒè¯
        val_metrics = validate(
            model, loader_val, criterion, device,
            use_robust_norm=use_robust_normalization
        )
        val_loss = val_metrics['total_loss']
        
        # è®°å½•å†å²
        train_losses_history.append(train_loss)
        val_losses_history.append(val_loss)
        train_ious_history.append(train_metrics['iou'])
        val_ious_history.append(val_metrics['iou'])
        learning_rates_history.append(opt.param_groups[0]['lr'])
        
        # æ‰“å°ç»“æœ
        print(f"  Train Loss: {train_loss:.6f} (BCE: {train_metrics['bce_loss']:.6f}, "
              f"Dice: {train_metrics['dice_loss']:.6f}, IoU: {train_metrics['iou']:.6f})")
        print(f"  Val Loss:   {val_loss:.6f} (BCE: {val_metrics['bce_loss']:.6f}, "
              f"Dice: {val_metrics['dice_loss']:.6f}, IoU: {val_metrics['iou']:.6f})")
        print(f"  å½“å‰å­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}")
        
        # ========== æ›´æ–°è¿‡æ‹Ÿåˆæ£€æµ‹å™¨ ==========
        improved, overfitting_signals = overfit_detector.update(epoch, train_loss, val_loss)
        
        # æ˜¾ç¤ºè¿‡æ‹Ÿåˆæ£€æµ‹å™¨çŠ¶æ€
        print(f"\n  {overfit_detector.get_summary()}")
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
        should_stop, stop_reasons = overfit_detector.should_stop(epoch)
        if overfitting_signals:
            print(f"  âš ï¸  è¿‡æ‹Ÿåˆä¿¡å·:")
            for signal in overfitting_signals:
                print(f"     - {signal}")
        
        
        # ========== ä¿å­˜æ¨¡å‹ ==========
        # å‡†å¤‡è°ƒåº¦å™¨çŠ¶æ€
        scheduler_state = None
        plateau_scheduler_state = None
        
        if scheduler_type != 'plateau':
            scheduler_state = scheduler.state_dict()
        if plateau_scheduler is not None:
            plateau_scheduler_state = plateau_scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        last_path = model_save_dir / 'model_last.pth'
        torch.save({
            'epoch': epoch,
            'model_state': model.state_dict(),
            'opt_state': opt.state_dict(),
            'scheduler_state': scheduler_state,
            'plateau_scheduler_state': plateau_scheduler_state,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_iou': train_metrics['iou'],
            'val_iou': val_metrics['iou'],
            'train_history': train_losses_history,
            'val_history': val_losses_history,
            'train_ious': train_ious_history,
            'val_ious': val_ious_history,
            'lr_history': learning_rates_history,
            'model_config': MODEL_CONFIG,
            'normalization_method': 'robust' if use_robust_normalization else 'traditional'
        }, str(last_path))
        
        # ä¹Ÿå†™ä¸€ä»½åˆ° latest/
        torch.save({
            'epoch': epoch,
            'model_state': model.state_dict(),
            'opt_state': opt.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_iou': train_metrics['iou'],
            'val_iou': val_metrics['iou'],
            'model_config': MODEL_CONFIG,
            'normalization_method': 'robust' if use_robust_normalization else 'traditional'
        }, str(latest_dir / 'model_last.pth'))
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
        save_best_loss = False
        save_best_iou = False
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_best_loss = True
        
        if val_metrics['iou'] > best_val_iou:
            best_val_iou = val_metrics['iou']
            save_best_iou = True
        
        if save_best_loss:
            best_path = model_save_dir / 'model_best_loss.pth'
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'train_history': train_losses_history,
                'val_history': val_losses_history,
                'train_ious': train_ious_history,
                'val_ious': val_ious_history,
                'lr_history': learning_rates_history,
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(best_path))
            
            # æ›´æ–° latest ä¸­çš„ best_loss
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(latest_dir / 'model_best_loss.pth'))
            
            print(f'  âœ… ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹ (val_loss: {val_loss:.6f}, val_iou: {val_metrics["iou"]:.6f})')
        
        if save_best_iou:
            best_iou_path = model_save_dir / 'model_best_iou.pth'
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'train_history': train_losses_history,
                'val_history': val_losses_history,
                'train_ious': train_ious_history,
                'val_ious': val_ious_history,
                'lr_history': learning_rates_history,
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(best_iou_path))
            
            # æ›´æ–° latest ä¸­çš„ best_iou
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'opt_state': opt.state_dict(),
                'val_loss': val_loss,
                'val_iou': val_metrics['iou'],
                'model_config': MODEL_CONFIG,
                'normalization_method': 'robust' if use_robust_normalization else 'traditional'
            }, str(latest_dir / 'model_best_iou.pth'))
            
            print(f'  âœ… ä¿å­˜æœ€ä½³IoUæ¨¡å‹ (val_iou: {val_metrics["iou"]:.6f}, val_loss: {val_loss:.6f})')
        
        # ========== å®šæœŸä¿å­˜è®­ç»ƒæ›²çº¿ ==========
        if epoch % 10 == 0 or epoch == epochs or should_stop:
            # æ‰©å±•çš„ç»˜å›¾å‡½æ•°
            def plot_extended_training_curves(train_losses, val_losses, train_ious, val_ious, 
                                            learning_rates=None, save_path=None):
                """ç»˜åˆ¶æ‰©å±•çš„è®­ç»ƒæ›²çº¿ï¼ŒåŒ…æ‹¬æŸå¤±å’ŒIoU"""
                if learning_rates is None:
                    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                else:
                    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
                
                epochs_range = range(1, len(train_losses) + 1)
                
                # æŸå¤±æ›²çº¿
                ax = axes[0, 0]
                ax.plot(epochs_range, train_losses, 'b-', label='Train Loss', alpha=0.7, linewidth=2)
                ax.plot(epochs_range, val_losses, 'r-', label='Val Loss', alpha=0.7, linewidth=2)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.set_title('Training and Validation Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # IoUæ›²çº¿
                ax = axes[0, 1]
                ax.plot(epochs_range, train_ious, 'g-', label='Train IoU', alpha=0.7, linewidth=2)
                ax.plot(epochs_range, val_ious, 'm-', label='Val IoU', alpha=0.7, linewidth=2)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('IoU')
                ax.set_title('Training and Validation IoU')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # æŸå¤±å·®è·æ›²çº¿
                ax = axes[1, 0]
                if len(train_losses) > 0 and len(val_losses) > 0:
                    gaps = [abs(t - v) for t, v in zip(train_losses, val_losses)]
                    ax.plot(epochs_range, gaps, 'c-', label='Train-Val Gap', alpha=0.7, linewidth=2)
                    ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Warning Threshold')
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('Gap')
                    ax.set_title('Train-Validation Gap')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                # IoUå·®è·æ›²çº¿
                ax = axes[1, 1]
                if len(train_ious) > 0 and len(val_ious) > 0:
                    iou_gaps = [abs(t - v) for t, v in zip(train_ious, val_ious)]
                    ax.plot(epochs_range, iou_gaps, 'y-', label='IoU Gap', alpha=0.7, linewidth=2)
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('IoU Gap')
                    ax.set_title('Train-Validation IoU Gap')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                # å­¦ä¹ ç‡æ›²çº¿
                if learning_rates is not None:
                    ax = axes[0, 2] if len(axes.shape) == 2 else axes[2]
                    ax.plot(epochs_range, learning_rates, color='purple', linestyle='-', 
                           label='Learning Rate', alpha=0.7, linewidth=2)
                    ax.set_xlabel('Epoch')
                    ax.set_ylabel('Learning Rate')
                    ax.set_title('Learning Rate Schedule')
                    ax.set_yscale('log')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                if save_path:
                    plt.savefig(save_path, dpi=150, bbox_inches='tight')
                plt.close()
            
            plot_path = model_save_dir / f'training_curve_epoch_{epoch}.png'
            plot_extended_training_curves(
                train_losses_history, val_losses_history, 
                train_ious_history, val_ious_history,
                learning_rates_history, plot_path
            )
            # å¦å­˜ä¸€ä»½åˆ° latest
            plot_extended_training_curves(
                train_losses_history, val_losses_history,
                train_ious_history, val_ious_history,
                learning_rates_history, 
                latest_dir / 'training_curve_latest.png'
            )
            print(f"  ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {plot_path}")
        
        # ========== æ¸…ç†ç¼“å­˜ ==========
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        # ========== æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ ==========
        if should_stop:
            print(f"\n{'!'*70}")
            print("æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œåœæ­¢è®­ç»ƒ!")
            print(f"åœæ­¢åŸå› :")
            for reason in stop_reasons:
                print(f"  - {reason}")
            print(f"æœ€ä½³æ¨¡å‹åœ¨ epoch {overfit_detector.best_epoch}, val_loss: {overfit_detector.best_val_loss:.6f}")
            print(f"{'!'*70}")
            
            # ä¿å­˜è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ
            overfit_info_path = model_save_dir / 'overfitting_report.txt'
            with open(overfit_info_path, 'w') as f:
                f.write("è¿‡æ‹Ÿåˆæ£€æµ‹æŠ¥å‘Š\n")
                f.write("="*50 + "\n")
                f.write(f"è®­ç»ƒåœæ­¢äº epoch {epoch}\n")
                f.write(f"æ€»è®­ç»ƒepochæ•°: {epochs}\n")
                f.write(f"å®é™…è®­ç»ƒepochæ•°: {epoch}\n")
                f.write(f"æœ€ä½³ epoch: {overfit_detector.best_epoch}\n")
                f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {overfit_detector.best_val_loss:.6f}\n")
                f.write(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}\n")
                f.write(f"åœæ­¢åŸå› :\n")
                for reason in stop_reasons:
                    f.write(f"  - {reason}\n")
                f.write("\nè®­ç»ƒå†å²:\n")
                f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss:.6f}\n")
                f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {val_loss:.6f}\n")
                f.write(f"  æœ€ç»ˆè®­ç»ƒIoU: {train_metrics['iou']:.6f}\n")
                f.write(f"  æœ€ç»ˆéªŒè¯IoU: {val_metrics['iou']:.6f}\n")
                f.write(f"  è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·: {abs(train_loss - val_loss):.6f}\n")
                f.write(f"  è®­ç»ƒ-éªŒè¯IoUå·®è·: {abs(train_metrics['iou'] - val_metrics['iou']):.6f}\n")
                f.write(f"  æœ€ç»ˆå­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}\n")
                f.write(f"  æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
                f.write(f"  æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
                f.write(f"  å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
            
            print(f"è¿‡æ‹ŸåˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {overfit_info_path}")
            
            # è®­ç»ƒæå‰åœæ­¢åï¼Œä¿å­˜æœ€ç»ˆè®­ç»ƒæ›²çº¿
            final_plot_path = model_save_dir / 'training_curve_final.png'
            plot_extended_training_curves(
                train_losses_history, val_losses_history,
                train_ious_history, val_ious_history,
                learning_rates_history, final_plot_path
            )
            break
    
    # ========== è®­ç»ƒå®Œæˆï¼ˆæœªæå‰åœæ­¢ï¼‰ ==========
    if not should_stop:
        print(f"\n{'='*70}")
        print("è®­ç»ƒå®Œæˆï¼Œæœªæ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        print(f"æœ€ä½³æ¨¡å‹åœ¨ epoch {overfit_detector.best_epoch}, val_loss: {overfit_detector.best_val_loss:.6f}")
        print(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}")
        print(f"{'='*70}")
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report_path = model_save_dir / 'training_report.txt'
        with open(report_path, 'w') as f:
            f.write("è®­ç»ƒæŠ¥å‘Š\n")
            f.write("="*50 + "\n")
            f.write(f"æ€»Epochs: {epochs}\n")
            f.write(f"å®ŒæˆEpochs: {epochs}\n")
            f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\n")
            f.write(f"æœ€ä½³éªŒè¯IoU: {best_val_iou:.6f}\n")
            f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss:.6f}\n")
            f.write(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_loss:.6f}\n")
            f.write(f"æœ€ç»ˆè®­ç»ƒIoU: {train_metrics['iou']:.6f}\n")
            f.write(f"æœ€ç»ˆéªŒè¯IoU: {val_metrics['iou']:.6f}\n")
            f.write(f"è®­ç»ƒ-éªŒè¯æŸå¤±å·®è·: {abs(train_loss - val_loss):.6f}\n")
            f.write(f"è®­ç»ƒ-éªŒè¯IoUå·®è·: {abs(train_metrics['iou'] - val_metrics['iou']):.6f}\n")
            f.write(f"ä¼˜åŒ–å™¨: {optimizer_type}\n")
            f.write(f"è°ƒåº¦å™¨: {scheduler_type}\n")
            f.write(f"åˆå§‹å­¦ä¹ ç‡: {lr}\n")
            f.write(f"æœ€ç»ˆå­¦ä¹ ç‡: {opt.param_groups[0]['lr']:.2e}\n")
            f.write(f"æƒé‡è¡°å‡: {weight_decay}\n")
            f.write(f"æ¢¯åº¦è£å‰ª: {max_grad_norm}\n")
            f.write(f"æ¨¡å‹: {MODEL_CONFIG['model_name']}\n")
            f.write(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}\n")
            f.write(f"AMPæ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}\n")
            f.write(f"æ‰¹å¤§å°: {batch_size}\n")
            f.write(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {accum_steps}\n")
            f.write(f"å½’ä¸€åŒ–æ–¹æ³•: {'é²æ£’Z-score(MAD)' if use_robust_normalization else 'ä¼ ç»ŸZ-score'}\n")
            f.write(f"å½’ä¸€åŒ–æˆªæ–­èŒƒå›´: {norm_clip_range}\n")
            f.write(f"æŸå¤±æƒé‡: BCE=0.3, Dice=0.7\n")
            f.write(f"å·¥ä½œçº¿ç¨‹: {workers}\n")
        
        print(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

if __name__ == '__main__':
    main()