import os
# å…è®¸é‡å¤çš„ OpenMP è¿è¡Œæ—¶
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau
from datetime import datetime
import random

# ============================================================================
# ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ (è¯·ç¡®ä¿è¿™äº›æ–‡ä»¶åœ¨æ‚¨çš„ç›®å½•ä¸‹)
# ============================================================================
try:
    from models.unet3d import UNet3D
    from dataloader import VolumeDataset
    from models.AERB3d import AERBUNet3DLight
    from models.AERB3d import AERBUNet3D
    from models.attention_unet3d import LightAttentionUNet3D
    from models.seunet3d import SEUNet3D
except ImportError:
    print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡å‹å®šä¹‰æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ models æ–‡ä»¶å¤¹åŠç›¸å…³ .py æ–‡ä»¶å­˜åœ¨ã€‚")
    # ä¸ºäº†é˜²æ­¢ä»£ç æŠ¥é”™ï¼Œè¿™é‡Œå®šä¹‰ç®€å•çš„å ä½ç¬¦ï¼Œå®é™…è¿è¡Œæ—¶è¯·å¿½ç•¥
    UNet3D = AERBUNet3DLight = LightAttentionUNet3D = SEUNet3D = VolumeDataset = None

# ============================================================================
# è®¾ç½®éšæœºç§å­
# ============================================================================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(42)

# ============================================================================
# æ¨¡å‹é…ç½®
# ============================================================================
MODEL_CONFIG = {
    "model_name": "attn_light",  # å¯é€‰: 'unet3d', 'aerb_light', 'attn_light', 'seunet3d', 'aerb3d'
    "in_channels": 1,
    "out_channels": 1,
    "base_channels": 16,
    "dropout_prob": 0.3,
    "pretrained_ckpt": None,
    "overfit_patience": 500  # æå‰åœæ­¢çš„è€å¿ƒå€¼
}

_MODEL_REGISTRY = {
    "unet3d": UNet3D,
    "aerb_light": AERBUNet3DLight,
    "attn_light": LightAttentionUNet3D,
    "seunet3d": SEUNet3D,
    'aerb3d': AERBUNet3D,
}

# ============================================================================
# ä¼˜åŒ–çš„æ··åˆæŸå¤±å‡½æ•° (Dice + BCE)
# ============================================================================
class DiceBCELoss(nn.Module):
    def __init__(self, weight_dice=0.5, weight_bce=0.5, eps=1e-6):
        """
        ç»„åˆ Dice Loss å’Œ BCE Loss
        """
        super(DiceBCELoss, self).__init__()
        self.weight_dice = weight_dice
        self.weight_bce = weight_bce
        self.eps = eps
        # ä½¿ç”¨ BCEWithLogitsLoss (å†…ç½® Sigmoidï¼Œæ•°å€¼æ›´ç¨³å®š)
        self.bce_func = nn.BCEWithLogitsLoss()

    def forward(self, logits, targets):
        # 1. è®¡ç®— BCE Loss
        if targets.dtype != torch.float32:
            targets = targets.float()
        bce_loss_val = self.bce_func(logits, targets)

        # 2. è®¡ç®— Dice Loss
        probs = torch.sigmoid(logits)
        # æ²¿ç”¨ä¹‹å‰çš„é€»è¾‘ï¼šåœ¨ (dim=1,2,3,4) ä¸Šæ±‚å’Œ
        num = 2 * (probs * targets).sum(dim=(1, 2, 3, 4))
        den = probs.sum(dim=(1, 2, 3, 4)) + targets.sum(dim=(1, 2, 3, 4))
        
        dice_score = (num + self.eps) / (den + self.eps)
        dice_loss_val = 1 - dice_score.mean()

        # 3. åŠ æƒç»„åˆ
        total_loss = (self.weight_dice * dice_loss_val) + (self.weight_bce * bce_loss_val)
        
        return total_loss, dice_loss_val, bce_loss_val


def augment_batch_data(x, y):
    """
    æ ¹æ®è®ºæ–‡å®ç°çš„ 3D åœ°éœ‡æ•°æ®å¢å¼º
    1. éšæœºæ²¿æ·±åº¦è½´ (Zè½´) ç¿»è½¬
    2. éšæœºç»•æ·±åº¦è½´ (Zè½´) æ—‹è½¬ 0, 90, 180, 270 åº¦
    
    å‚æ•°:
        x: åœ°éœ‡æ•°æ® Tensor, å½¢çŠ¶ (B, C, D, H, W) æˆ– (B, D, H, W)
        y: æ ‡ç­¾æ•°æ® Tensor, å½¢çŠ¶åŒ x
    """
    # ç¡®ä¿æ˜¯ Tensor
    if not isinstance(x, torch.Tensor):
        x = torch.from_numpy(np.asarray(x))
    if not isinstance(y, torch.Tensor):
        y = torch.from_numpy(np.asarray(y))

    # è·å–ç»´åº¦ç´¢å¼•
    # å‡è®¾æ ‡å‡† 5D è¾“å…¥: (Batch, Channel, Depth, Height, Width) -> (B, C, Z, Y, X)
    # Depth æ˜¯ dim=2, H-W å¹³é¢æ˜¯ dim=[3, 4]
    if x.ndim == 5:
        z_axis = 2
        plane_axes = [3, 4]
    elif x.ndim == 4: # (Batch, Depth, Height, Width)
        z_axis = 1
        plane_axes = [2, 3]
    else:
        return x, y # ç»´åº¦ä¸å¯¹ï¼Œè·³è¿‡å¢å¼º

    # --- 1. å‚ç›´ç¿»è½¬ (Vertical Flip / Z-flip) ---
    if random.random() > 0.5:
        x = torch.flip(x, dims=[z_axis])
        y = torch.flip(y, dims=[z_axis])

    # --- 2. ç»• Z è½´æ—‹è½¬ (Rotation 90/180/270) ---
    # è¿™ç›¸å½“äºåœ¨ H-W (Inline-Crossline) å¹³é¢ä¸Šæ—‹è½¬
    k = random.randint(0, 3) # ç”Ÿæˆ 0, 1, 2, 3
    if k > 0:
        x = torch.rot90(x, k, dims=plane_axes)
        y = torch.rot90(y, k, dims=plane_axes)

    return x, y



# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€å½’ä¸€åŒ–ã€IoU
# ============================================================================
def get_optimizer(model, lr=1e-4, optimizer_type='adamw', weight_decay=1e-3):
    if optimizer_type.lower() == 'adamw':
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_decay)
    elif optimizer_type.lower() == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_decay)
    else:
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=True)
    return optimizer

def get_adaptive_scheduler(optimizer, warmup_epochs=None, total_epochs=400, scheduler_type='auto'):
    if warmup_epochs is None:
        if total_epochs <= 30: warmup_epochs = max(2, total_epochs // 10)
        elif total_epochs <= 100: warmup_epochs = max(5, total_epochs // 10)
        elif total_epochs <= 200: warmup_epochs = max(10, total_epochs // 15)
        else: warmup_epochs = max(15, total_epochs // 20)
    
    if scheduler_type == 'auto':
        # [ä¿®æ”¹] å»ºè®®å¯¹äº 400 epoch è¿™ç§é•¿è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨å•å‘¨æœŸä½™å¼¦ 'cosine'
        # è¿™æ ·å­¦ä¹ ç‡ä¼šå¹³æ»‘ä¸‹é™ï¼Œä¸ä¼šåœ¨åæœŸçªç„¶åå¼¹ï¼Œæœ‰åˆ©äºæ¨¡å‹æ”¶æ•›ç¨³å®š
        scheduler_type = 'cosine' 
    
    print(f"è°ƒåº¦é…ç½®: {total_epochs} epochs -> warmup={warmup_epochs}, scheduler={scheduler_type}")
    
    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)
    
    if scheduler_type == 'cosine_warm':
        # å¦‚æœä½ éè¦ç”¨çƒ­é‡å¯ï¼Œå»ºè®®æŠŠ T_0 è®¾å°ä¸€ç‚¹ï¼Œæ¯”å¦‚ 50
        T_0 = 50 
        main_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=2, eta_min=1e-6)
    elif scheduler_type == 'cosine':
        # è¿™æ˜¯æœ€ç¨³çš„ç­–ç•¥
        main_scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs - warmup_epochs, eta_min=1e-6)
    else:
        step_size = max(20, total_epochs // 5)
        main_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.5)
    
    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])
    return scheduler, None


def _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)):
    """é²æ£’å½’ä¸€åŒ– (MAD)"""
    if isinstance(x, torch.Tensor): x_np = x.cpu().numpy()
    else: x_np = np.asarray(x)
    original_shape = x_np.shape
    
    if x_np.ndim == 5: x_reshaped = x_np.reshape(original_shape[0] * original_shape[1], *original_shape[2:])
    elif x_np.ndim == 4: x_reshaped = x_np
    else: raise ValueError('Unsupported ndim')
    
    x_norm = np.zeros_like(x_reshaped, dtype=np.float32)
    for i in range(x_reshaped.shape[0]):
        sample = x_reshaped[i]
        center = np.median(sample)
        if use_mad:
            mad = np.median(np.abs(sample - center))
            scale = mad * 1.4826 if mad > 1e-6 else 1.0
        else:
            scale = np.std(sample)
            if scale < 1e-6: scale = 1.0
        sample_norm = (sample - center) / scale
        if clip_range: sample_norm = np.clip(sample_norm, clip_range[0], clip_range[1])
        x_norm[i] = sample_norm
        
    if x_np.ndim == 5: x_norm = x_norm.reshape(original_shape)
    return torch.from_numpy(x_norm).float()

def _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)):
    """ä¼ ç»Ÿå½’ä¸€åŒ– (Mean/Std)"""
    if isinstance(x, torch.Tensor): x = x.cpu().numpy()
    x = np.asarray(x)
    spatial_axes = (2, 3, 4) if x.ndim == 5 else (1, 2, 3)
    mean = x.mean(axis=spatial_axes, keepdims=True)
    std = x.std(axis=spatial_axes, keepdims=True)
    std[std < 1e-6] = 1.0
    x_norm = (x - mean) / std
    if clip_range: x_norm = np.clip(x_norm, clip_range[0], clip_range[1])
    return torch.from_numpy(x_norm).float()

def _batch_iou(logits, targets, threshold=0.5, eps=1e-7):
    preds = (torch.sigmoid(logits) > threshold)
    targets_bool = targets.bool()
    intersection = (preds & targets_bool).sum(dim=(1, 2, 3, 4)).float()
    union = (preds | targets_bool).sum(dim=(1, 2, 3, 4)).float()
    iou = (intersection + eps) / (union + eps)
    return iou.mean()

# ============================================================================
# è¿‡æ‹Ÿåˆæ£€æµ‹å™¨
# ============================================================================
class OverfittingDetector:
    def __init__(self, patience=15, min_delta=1e-4, gap_threshold=0.5):
        self.patience = patience
        self.min_delta = min_delta
        self.gap_threshold = gap_threshold
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.counter = 0
        self.train_losses = []
        self.val_losses = []
        self.train_val_gaps = []

    def update(self, epoch, train_loss, val_loss):
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_val_gaps.append(abs(train_loss - val_loss))
        
        if val_loss < self.best_val_loss - self.min_delta:
            self.best_val_loss = val_loss
            self.best_epoch = epoch
            self.counter = 0
            improved = True
        else:
            self.counter += 1
            improved = False
        return improved, self.check_overfitting()

    def check_overfitting(self):
        signals = []
        if len(self.val_losses) >= 5:
            recent = self.val_losses[-5:]
            if np.polyfit(range(5), recent, 1)[0] > 0.01: signals.append("éªŒè¯æŸå¤±è¿ç»­ä¸Šå‡")
        if self.train_val_gaps[-1] > self.gap_threshold: signals.append("è®­ç»ƒ-éªŒè¯å·®è·è¿‡å¤§")
        return signals

    def should_stop(self, epoch):
        return (self.counter >= self.patience), self.check_overfitting()
    
    def get_summary(self):
        return f"Best: {self.best_val_loss:.6f} (Epoch {self.best_epoch}) | Patience: {self.counter}/{self.patience}"

# ============================================================================
# ç»˜å›¾å·¥å…·
# ============================================================================
def plot_extended_training_curves(train_losses, val_losses, train_ious, val_ious, 
                                  learning_rates=None, save_path=None):
    """ç»˜åˆ¶æ‰©å±•çš„è®­ç»ƒæ›²çº¿"""
    if learning_rates is None:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    else:
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    epochs_range = range(1, len(train_losses) + 1)
    
    # Loss
    ax = axes[0, 0]
    ax.plot(epochs_range, train_losses, 'b-', label='Train', alpha=0.7)
    ax.plot(epochs_range, val_losses, 'r-', label='Val', alpha=0.7)
    ax.set_title('Loss')
    ax.legend(); ax.grid(True, alpha=0.3)
    
    # IoU
    ax = axes[0, 1]
    ax.plot(epochs_range, train_ious, 'g-', label='Train', alpha=0.7)
    ax.plot(epochs_range, val_ious, 'm-', label='Val', alpha=0.7)
    ax.set_title('IoU')
    ax.legend(); ax.grid(True, alpha=0.3)
    
    # Gap
    ax = axes[1, 0]
    if len(train_losses) > 0:
        gaps = [abs(t - v) for t, v in zip(train_losses, val_losses)]
        ax.plot(epochs_range, gaps, 'c-', label='Train-Val Gap')
        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
        ax.set_title('Loss Gap')
        ax.legend(); ax.grid(True, alpha=0.3)

    # IoU Gap
    ax = axes[1, 1]
    if len(train_ious) > 0:
        iou_gaps = [abs(t - v) for t, v in zip(train_ious, val_ious)]
        ax.plot(epochs_range, iou_gaps, 'y-', label='IoU Gap')
        ax.set_title('IoU Gap')
        ax.legend(); ax.grid(True, alpha=0.3)
        
    # LR
    if learning_rates is not None:
        ax = axes[0, 2] if len(axes.shape) == 2 else axes[2]
        ax.plot(epochs_range, learning_rates, color='purple')
        ax.set_yscale('log')
        ax.set_title('Learning Rate')
        ax.grid(True, alpha=0.3)
        
    plt.tight_layout()
    if save_path: plt.savefig(save_path, dpi=100)
    plt.close()

# ============================================================================
# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# ============================================================================
def train_epoch(model, loader, opt, criterion, device, scaler=None, accum_steps=1, max_grad_norm=1.0, use_robust_norm=True):
    model.train()
    stats = {'loss': 0.0, 'bce': 0.0, 'dice': 0.0, 'iou': 0.0}
    opt.zero_grad()
    
    for step, (x, y) in enumerate(tqdm(loader, desc='train', leave=False), start=1):
        x, y = augment_batch_data(x, y)
        if use_robust_norm:
            x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).to(device)
        else:
            x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).to(device)
        y = y.float().to(device)

        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(x)
                loss, dice_val, bce_val = criterion(logits, y)
                batch_iou = _batch_iou(logits, y)
            scaler.scale(loss / accum_steps).backward()
        else:
            logits = model(x)
            loss, dice_val, bce_val = criterion(logits, y)
            batch_iou = _batch_iou(logits, y)
            (loss / accum_steps).backward()

        bs = x.size(0)
        stats['loss'] += loss.item() * bs
        stats['bce'] += bce_val.item() * bs
        stats['dice'] += dice_val.item() * bs
        stats['iou'] += batch_iou.item() * bs

        if step % accum_steps == 0 or step == len(loader):
            if scaler is not None:
                scaler.unscale_(opt)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(opt)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                opt.step()
            opt.zero_grad()
            
    total = len(loader.dataset)
    return {k: v / total for k, v in stats.items()}

def validate(model, loader, criterion, device, use_robust_norm=True):
    model.eval()
    stats = {'loss': 0.0, 'bce': 0.0, 'dice': 0.0, 'iou': 0.0}
    
    with torch.no_grad():
        for x, y in tqdm(loader, desc='val', leave=False):
            if use_robust_norm:
                x = _robust_normalize_tensor_batch(x, use_mad=True, clip_range=(-4.0, 4.0)).to(device)
            else:
                x = _traditional_normalize_tensor_batch(x, clip_range=(-3.0, 3.0)).to(device)
            y = y.float().to(device)
            
            logits = model(x)
            loss, dice_val, bce_val = criterion(logits, y)
            batch_iou = _batch_iou(logits, y)
            
            bs = x.size(0)
            stats['loss'] += loss.item() * bs
            stats['bce'] += bce_val.item() * bs
            stats['dice'] += dice_val.item() * bs
            stats['iou'] += batch_iou.item() * bs
            
    total = len(loader.dataset)
    return {k: v / total for k, v in stats.items()}

def build_model_from_config(cfg):
    name = cfg.get("model_name", "unet3d")
    cls = _MODEL_REGISTRY.get(name)
    if cls is None: raise ValueError(f"Unknown model: {name}")
    
    kwargs = {k: v for k, v in cfg.items() if k in cls.__init__.__code__.co_varnames}
    model = cls(**kwargs)
    
    ckpt = cfg.get("pretrained_ckpt")
    if ckpt and os.path.exists(ckpt):
        try:
            sd = torch.load(ckpt, map_location="cpu")
            if "model_state" in sd: sd = sd["model_state"]
            model.load_state_dict(sd, strict=False)
            print("å·²åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚")
        except Exception as e:
            print(f"æƒé‡åŠ è½½å¤±è´¥: {e}")
    return model

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    # ========== 1. æ ¸å¿ƒè®­ç»ƒé…ç½® ==========
    WEIGHT_BCE = 0.5   
    WEIGHT_DICE = 0.5  
    
    USE_ROBUST_NORM = True 
    NORM_CLIP = (-4.0, 4.0) if USE_ROBUST_NORM else (-3.0, 3.0)

    EPOCHS = 400
    BATCH_SIZE = 8
    LR = 1e-4
    WORKERS = 4
    
    # [æ–°å¢] .dat æ–‡ä»¶ä¸“ç”¨é…ç½® (å¿…é¡»ä¸ä½ çš„æ•°æ®ä¸€è‡´)
    DAT_CONFIG = {
        'dat_dtype': 'float32',       # æ•°æ®ç±»å‹
        'dat_shape': (128, 128, 128), # æ•°æ®å°ºå¯¸ (Z, H, W)
        'dat_order': 'C'              # å­—èŠ‚åº
    }
    
    # è·¯å¾„é…ç½®
    root = Path('.')
    train_data = root / 'train' / 'seis'
    train_label = root / 'train' / 'fault'
    val_data = root / 'prediction' / 'seis'
    val_label = root / 'prediction' / 'fault'
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Running on {device}")
    
    # ========== 2. æ•°æ®å‡†å¤‡ (å·²ä¿®å¤) ==========
    if not train_data.exists():
        print(f"âš ï¸ é”™è¯¯: è®­ç»ƒæ•°æ®æœªæ‰¾åˆ° {train_data}")
        return 

    # [ä¿®å¤] ä¼ å…¥ **DAT_CONFIG è§£åŒ…å‚æ•°
    ds_train = VolumeDataset(str(train_data), str(train_label), **DAT_CONFIG)
    
    if val_data.exists():
        ds_val = VolumeDataset(str(val_data), str(val_label), **DAT_CONFIG)
    else:
        print("âš ï¸ æç¤º: æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œè‡ªåŠ¨åˆ’åˆ† 10% è®­ç»ƒé›†ç”¨äºéªŒè¯ã€‚")
        train_len = int(0.9 * len(ds_train))
        ds_train, ds_val = torch.utils.data.random_split(ds_train, [train_len, len(ds_train)-train_len])
    
    loader_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=True)
    loader_val = DataLoader(ds_val, batch_size=1, shuffle=False, num_workers=min(2, WORKERS))
    
    # ========== 3. æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ==========
    model = build_model_from_config(MODEL_CONFIG).to(device)
    opt = get_optimizer(model, lr=LR, optimizer_type='adamw')
    
    criterion = DiceBCELoss(weight_dice=WEIGHT_DICE, weight_bce=WEIGHT_BCE).to(device)
    
    scheduler, plateau_scheduler = get_adaptive_scheduler(opt, total_epochs=EPOCHS)

    detector = OverfittingDetector(patience=MODEL_CONFIG.get("overfit_patience", 500))
    
    scaler = torch.cuda.amp.GradScaler() if (device.type == 'cuda') else None
    
    # ========== 4. ä¿å­˜ç›®å½•å‡†å¤‡ ==========
    checkpoints_root = Path('checkpoints2')
    model_tag = MODEL_CONFIG["model_name"]
    base_ch = MODEL_CONFIG.get('base_channels')
    if base_ch and f"_c{base_ch}" not in model_tag: model_tag = f"{model_tag}_c{base_ch}"
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_save_dir = checkpoints_root / model_tag / timestamp
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    latest_dir = checkpoints_root / model_tag / 'latest'
    latest_dir.mkdir(parents=True, exist_ok=True)
    
    with open(model_save_dir / 'training_config.txt', 'w') as f:
        f.write(f"Model: {model_tag}\nWeights: BCE={WEIGHT_BCE}, Dice={WEIGHT_DICE}\n")
        f.write(f"Norm: Robust={USE_ROBUST_NORM}, Clip={NORM_CLIP}\n")
        f.write(f"LR: {LR}, Batch: {BATCH_SIZE}, Epochs: {EPOCHS}\n")
        f.write(f"Dat Config: {DAT_CONFIG}\n") # è®°å½•æ•°æ®é…ç½®

    history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], 'lr': []}
    best_val_iou = 0.0

    print(f"\nå¼€å§‹è®­ç»ƒ | Loss Weights: BCE={WEIGHT_BCE}/Dice={WEIGHT_DICE} | Norm: {NORM_CLIP}\n")
    
    # ========== 5. è®­ç»ƒå¾ªç¯ ==========
    for epoch in range(1, EPOCHS + 1):
        print(f"Epoch {epoch}/{EPOCHS}")
        
        t_metrics = train_epoch(model, loader_train, opt, criterion, device, scaler=scaler, use_robust_norm=USE_ROBUST_NORM)
        v_metrics = validate(model, loader_val, criterion, device, use_robust_norm=USE_ROBUST_NORM)
        
        history['train_loss'].append(t_metrics['loss'])
        history['val_loss'].append(v_metrics['loss'])
        history['train_iou'].append(t_metrics['iou'])
        history['val_iou'].append(v_metrics['iou'])
        history['lr'].append(opt.param_groups[0]['lr'])
        
        print(f" Train | Loss: {t_metrics['loss']:.4f} (B:{t_metrics['bce']:.3f}, D:{t_metrics['dice']:.3f}) | IoU: {t_metrics['iou']:.4f}")
        print(f" Val   | Loss: {v_metrics['loss']:.4f} (B:{v_metrics['bce']:.3f}, D:{v_metrics['dice']:.3f}) | IoU: {v_metrics['iou']:.4f}")
        
        improved, signals = detector.update(epoch, t_metrics['loss'], v_metrics['loss'])
        print(f" {detector.get_summary()}")
        
        scheduler.step()
        
        last_state = {
            'epoch': epoch, 'model_state': model.state_dict(), 'opt_state': opt.state_dict(),
            'train_loss': t_metrics['loss'], 'val_loss': v_metrics['loss'],
            'train_iou': t_metrics['iou'], 'val_iou': v_metrics['iou'],
            'history': history, 'config': MODEL_CONFIG
        }
        
        torch.save(last_state, model_save_dir / 'model_last.pth')
        torch.save(last_state, latest_dir / 'model_last.pth')
        
        if improved:
            torch.save(last_state, model_save_dir / 'model_best_loss.pth')
            torch.save(last_state, latest_dir / 'model_best_loss.pth')
            print(f" âœ… Saved Best Loss Model")

        if v_metrics['iou'] > best_val_iou:
            best_val_iou = v_metrics['iou']
            torch.save(last_state, model_save_dir / 'model_best_iou.pth')
            torch.save(last_state, latest_dir / 'model_best_iou.pth')
            print(f" âœ… Saved Best IoU Model ({best_val_iou:.4f})")
            
        if epoch % 5 == 0 or epoch == EPOCHS:
            plot_path = model_save_dir / f'training_curve_epoch_{epoch}.png'
            plot_extended_training_curves(
                history['train_loss'], history['val_loss'], 
                history['train_iou'], history['val_iou'], 
                history['lr'], plot_path
            )
            plot_extended_training_curves(
                history['train_loss'], history['val_loss'], 
                history['train_iou'], history['val_iou'], 
                history['lr'], latest_dir / 'training_curve_latest.png'
            )

        should_stop, _ = detector.should_stop(epoch)
        if should_stop:
            print("ğŸ›‘ Early stopping triggered.")
            break

    print("è®­ç»ƒç»“æŸã€‚")
if __name__ == '__main__':
    main()